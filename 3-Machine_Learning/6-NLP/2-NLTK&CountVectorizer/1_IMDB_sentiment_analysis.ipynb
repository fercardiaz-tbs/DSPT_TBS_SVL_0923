{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del sentimiento en IMDB\n",
    "\n",
    "Los datos se dividen en partes iguales: 25.000 reseñas para el entrenamiento y 25.000 para probar el clasificador. Además, cada conjunto tiene 12,5k críticas positivas y 12,5k negativas.\n",
    "\n",
    "IMDb permite a los usuarios puntuar las películas en una escala del 1 al 10. Para etiquetar estas reseñas, el encargado de los datos etiquetó todo lo que tuviera ≤ 4 estrellas como negativo y todo lo que tuviera ≥ 7 estrellas como positivo. Las reseñas con 5 o 6 estrellas se omitieron.\n",
    "\n",
    "**Importar las bibliotecas necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = []\n",
    "for line in open(os.getcwd() + '/data/imbd_train.txt', 'r', encoding='latin1'):\n",
    "    \n",
    "    reviews_train.append(line.strip())\n",
    "    \n",
    "reviews_test = []\n",
    "for line in open(os.getcwd() + '/data/imbd_test.txt', 'r', encoding='latin1'):\n",
    "    \n",
    "    reviews_test.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print('####################')\n",
    "    print(reviews_train[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ver uno de los elementos de la lista**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reviews_train))\n",
    "print(len(reviews_test))\n",
    "reviews_train[5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El texto en bruto es bastante desordenado para hacer una revisión de esta manera, por lo que antes de que podamos hacer cualquier análisis tenemos que limpiar las cosas\n",
    "\n",
    "\n",
    "**Utilizar expresiones regulares para eliminar los caracteres no textuales y las etiquetas html**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remover todos los signos de puntuación, exclamaciones...\n",
    "# Tb pasamos a minuscula y nos cargamos etiquetas HTML\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    # Para todas las reviews en minuscula, sustituye algunas cosas por espacio y otras por vacio.\n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# Reviews tras aplicar la limpieza\n",
    "reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "reviews_test_clean = preprocess_reviews(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_clean[5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "Para que estos datos tengan sentido para nuestro algoritmo de aprendizaje automático, tendremos que convertir cada opinión en una representación numérica, lo que llamamos vectorización.\n",
    "\n",
    "La forma más sencilla de hacerlo es crear una matriz muy grande con una columna para cada palabra única del corpus (en nuestro caso, el corpus son las 50.000 reseñas). A continuación, transformamos cada opinión en una fila que contiene 0 y 1, donde 1 significa que la palabra del corpus correspondiente a esa columna aparece en esa opinión. Dicho esto, cada fila de la matriz será muy escasa (mayoritariamente ceros). Este proceso también se conoce como codificación en caliente. Utilice el método *CountVectorizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si aparece una palabra en una review, le pone un 1. Da igual que aparezca 100 veces, no cuenta. Xq binary=True\n",
    "# Solo pone 1s cuando detecta una palabra en una review\n",
    "baseline_vectorizer = CountVectorizer(binary=True)\n",
    "baseline_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "# Reviews en formato vector de palabras. El mismo vectorizador a test, tiene que mantener la estructura\n",
    "X_baseline = baseline_vectorizer.transform(reviews_train_clean)\n",
    "X_test_baseline = baseline_vectorizer.transform(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(X_baseline.shape)\n",
    "\n",
    "# Asigna un numero según el orden de aparicion\n",
    "baseline_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_c = CountVectorizer()\n",
    "vectorizer_c.fit(reviews_train_clean)\n",
    "\n",
    "# Ya no es binaria la aparicion, sino un conteo por palabra\n",
    "X_baseline_c = vectorizer_c.transform(reviews_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_baseline_c.shape)\n",
    "print(len(vectorizer_c.get_feature_names())) # Las mismas\n",
    "# X_baseline_c.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz demasiado grande como para que numpy la imprima por pantalla\n",
    "X_baseline_c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Baseline Model\n",
    "\n",
    "Entrenar un modelo de Regresión Logística después de transformar los datos con CountVectorized\n",
    "\n",
    "* Son fáciles de interpretar\n",
    "* Los modelos lineales tienden a funcionar bien en conjuntos de datos dispersos como éste.\n",
    "* Aprenden muy rápido en comparación con otros algoritmos.\n",
    "\n",
    "Probar modelos con valores de C de [0.01, 0.05, 0.25, 0.5, 1] y ver cual es el mejor valor para C, y calcular la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Los comentarios vienen ordenados. Los primeros 12,5k son negativos\n",
    "# A test le ocurre lo mismo\n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "# target = []\n",
    "# for i in range(25000):\n",
    "#     if i < 12500: \n",
    "#         target.append(1)\n",
    "#     else:\n",
    "#         target.append(0)\n",
    "\n",
    "def train_model(X_TRAIN, X_TEST):\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    \n",
    "    params = {\n",
    "        'C': [0.01, 0.05, 0.25, 0.5, 1]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(lr, params, cv=5)\n",
    "    grid.fit(X_TRAIN, target)\n",
    "\n",
    "    print (\"Final Accuracy: %s\" % accuracy_score(target, grid.best_estimator_.predict(X_TEST)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(X_baseline, X_test_baseline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar las Stop Words\n",
    "\n",
    "Las stop words son palabras muy comunes como \"si\", \"pero\", \"nosotros\", \"él\", \"ella\" y \"ellos\". Normalmente podemos eliminar estas palabras sin cambiar la semántica de un texto y hacerlo a menudo (aunque no siempre) mejora el rendimiento de un modelo. La eliminación de estas palabras de parada resulta mucho más útil cuando empezamos a utilizar secuencias de palabras más largas como características del modelo (véanse los n-gramas más adelante).\n",
    "\n",
    "Antes de aplicar el CountVectorized, vamos a eliminar las stopwords, incluidas en nltk.corpus\n",
    "\n",
    "A continuación, aplica el CountVectorizer y entrena el modelo de regresión logística para obtener la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que bajarse las stopwords de nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para visualizar los stopwords de inglés\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para visualizarlas en español\n",
    "stopwords.words('spanish')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para visualizarlas en español\n",
    "stopwords.words('chinese')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('catalan')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('basque')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Aplicamos la eliminacion de las palabras directamente sobre las reviews\n",
    "# Demasiado manual. Mejor sobre el CountVectorizer (ver abajo)\n",
    "english_stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        \n",
    "        # Para cada review elimina las stopwords, y separa todas las palabras por espacio\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() if word not in english_stop_words])\n",
    "        )\n",
    "        \n",
    "    return removed_stop_words\n",
    "\n",
    "# Se lo aplicamos antes de vectorizar\n",
    "no_stop_words_train = remove_stop_words(reviews_train_clean)\n",
    "no_stop_words_test = remove_stop_words(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vectorizamos tras eliminar las stop words\n",
    "Ver docu, tiene cosas interesantes como lowercase=True. Lo hace antes de vectorizar, \n",
    "o el argumento stopwords\n",
    "'''\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(no_stop_words_train)\n",
    "\n",
    "X = cv.transform(no_stop_words_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se aplica el mismo a test\n",
    "X_test = cv.transform(no_stop_words_test)\n",
    "\n",
    "# Y entrenamos\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_baseline tras aplicar el vectorizador tal cual en los datos\n",
    "X tras aplicar el vectorizador despues de eliminar las stop words. No se carga muchas\n",
    "'''\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "El resultado de este codigo es practicamente igual que el anterior, pero elimina más stopwords\n",
    "'''\n",
    "\n",
    "cv = CountVectorizer(binary=True,\n",
    "                     stop_words = english_stop_words)\n",
    "\n",
    "cv.fit(reviews_train_clean)\n",
    "\n",
    "X = cv.transform(reviews_train_clean)\n",
    "X_test = cv.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_baseline tras aplicar el vectorizador tal cual en los datos\n",
    "X tras aplicar el vectorizador despus de eliminar las stop words\n",
    "En este caso elimina más, el countvectorizer tokeniza mejor las palabras\n",
    "de lo que lo hemos hecho nosotros en la funcion remove_stop_words. Por ejemplo \"it's\" serian dos palabras\n",
    "'''\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** En la práctica, una manera más fácil de eliminar las stop words es simplemente utilizar el argumento stop_words con cualquiera de las clases 'Vectorizer' de scikit-learn. Si quieres usar la lista completa de stop words de NLTK puedes usar stop_words='english'. En la práctica he encontrado que el uso de la lista de NLTK en realidad disminuye mi rendimiento porque es demasiado amplia, por lo que normalmente proporciono mi propia lista de palabras. Por ejemplo, stop_words=['in','of','at','a','the'] ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un paso habitual en el preprocesamiento de textos es normalizar las palabras del corpus intentando convertir todas las formas de una palabra en una sola. Para ello existen dos métodos: el Stemming y la Lemmatization.\n",
    "\n",
    "# Stemming\n",
    "\n",
    "El \"stemming\" se considera el método de normalización más tosco o de fuerza bruta (aunque esto no significa necesariamente que vaya a dar peores resultados). Hay varios algoritmos, pero en general todos utilizan reglas básicas para cortar los extremos de las palabras.\n",
    "\n",
    "NLTK tiene varias implementaciones de algoritmos de stemming. Nosotros usaremos el stemmer Porter. Los más usados:\n",
    "* PorterStemmer\n",
    "* SnowballStemmer\n",
    "\n",
    "Aplicar un PoterStemmer, vectorizar, y entrenar el modelo de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "El stemmer se aplica sobre cada palabra. Las recorta eliminando plurales y tiempos verbales\n",
    "Modifica muy poco cada palabra\n",
    "'''\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'fly','flight', 'flown', 'dies', 'die', 'mules', 'denied', 'deny',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer', 'colonizing',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "plurals = ['caresses', 'flies', 'fly','flight', 'dies', 'mules', 'denied', 'deny',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "plurals = ['recorrer', 'corriendo', 'correlación', 'correré', 'casas', 'casero', 'caso', 'playa', 'volando', 'volar', 'volveré']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos a mano. Los stemmers no eliminan palabras, solo quitan sufijos, y ahora habrá más palabras que sean iguales\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def get_stemmed_text(corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "stemmed_reviews_train = get_stemmed_text(reviews_train_clean)\n",
    "stemmed_reviews_test = get_stemmed_text(reviews_test_clean)\n",
    "\n",
    "cv = CountVectorizer(binary=True, stop_words = english_stop_words)\n",
    "cv.fit(stemmed_reviews_train)\n",
    "\n",
    "X_stem = cv.transform(stemmed_reviews_train)\n",
    "X_test = cv.transform(stemmed_reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(X_stem, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No elimina palabras. Solo recorta sufijos y agrupa tipos de palabras.\n",
    "# Como resultado dará menos palabras debido al agrupado. Se carga unas cuantas letras de las palabras\n",
    "print(X_baseline.shape)\n",
    "print(X_stem.shape)\n",
    "print(\"Diff X normal y X tras stemmer y vectorización:\", X_baseline.shape[1] - X_stem.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "La Lemmatization consiste en identificar la parte del discurso de una palabra determinada y, a continuación, aplicar reglas más complejas para transformar la palabra en su verdadera raíz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "La diferencia con el stemming es que la lematización tiene en cuenta la morfología\n",
    "de la palabra, sustituyendola por la raiz, no recortándola. Y no es tan restrictivo como el stemming.\n",
    "Necesita un buen diccionario con mapeos, como wordnet\n",
    "\n",
    "En nltk no hay lematizadores en español. Habria que bajarse algun paquete como pip install es-lemmatizer\n",
    "'''\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "plurals = ['caresses', 'flies','fly','flight', 'dies', 'mules', 'studies',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [lemmatizer.lemmatize(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatized_text(corpus):\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "# Lematizamos las reviews\n",
    "lemmatized_reviews_train = get_lemmatized_text(reviews_train_clean)\n",
    "lemmatized_reviews_test = get_lemmatized_text(reviews_test_clean)\n",
    "\n",
    "# Vectorizamos con conteo tras lematizar\n",
    "cv = CountVectorizer(binary=True, stop_words = english_stop_words)\n",
    "cv.fit(lemmatized_reviews_train)\n",
    "\n",
    "X = cv.transform(lemmatized_reviews_train)\n",
    "X_test = cv.transform(lemmatized_reviews_test)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina menos que con el stemmer. Normal, el stemmer recorta mucho del sufijo\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "Podemos añadir potencialmente más poder predictivo a nuestro modelo añadiendo también secuencias de dos o tres palabras (bigramas o trigramas). Por ejemplo, si una crítica tuviera la secuencia de tres palabras \"no me gustó la película\", sólo consideraríamos estas palabras individualmente con un modelo de unigramas y probablemente no captaríamos que se trata de un sentimiento negativo, porque la palabra \"me gustó\" por sí sola va a estar muy correlacionada con una crítica positiva.\n",
    "\n",
    "La biblioteca scikit-learn hace que sea muy fácil jugar con esto. Sólo tiene que utilizar el argumento ngram_range con cualquiera de las clases 'Vectorizer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"didn't love music at all my love\"\n",
    "\n",
    "one = ngrams(sentence.split(), 1)\n",
    "two = ngrams(sentence.split(), 2)\n",
    "three = ngrams(sentence.split(), 3)\n",
    "\n",
    "for grams in one:\n",
    "  print(grams)\n",
    "\n",
    "for grams in two:\n",
    "  print(grams)\n",
    "print('###############')\n",
    "for grams in three:\n",
    "  print(grams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Puede ser bigramas si ngram_range=(2,2), o trigramas (3,3)...\n",
    "Algunas palabras las elimina, como 'a'. Cuidado con eso a la hora de hacer el conteo\n",
    "ngram_range=(1, 3) significa las palabras por separado, los bigramas y los trigramas\n",
    "Ojo que esto aumenta muchisimo el espacio de features\n",
    "'''\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                   ngram_range=(1, 2))\n",
    "\n",
    "vector = ngram_vectorizer.fit_transform([sentence]).toarray()\n",
    "print(vector)\n",
    "print(len(vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Va como argumento del CountVectorizer\n",
    "'''\n",
    "ngram_vectorizer = CountVectorizer(binary=True, stop_words = english_stop_words,\n",
    "                                   ngram_range=(1, 2))\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade 1448047 n gramas. Cuanto mas ngramas, mayor será el espacio de features\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Va como argumento del CountVectorizer\n",
    "'''\n",
    "ngram_vectorizer = CountVectorizer(binary=True, stop_words = english_stop_words,\n",
    "                                   ngram_range=(2, 2))\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])\n",
    "\n",
    "# train_model(X, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Otra forma habitual de representar cada documento de un corpus es utilizar el estadístico tf-idf (frecuencia de términos-frecuencia inversa de documentos) para cada palabra, que es un factor de ponderación que podemos utilizar en lugar de las representaciones binarias o de recuento de palabras.\n",
    "\n",
    "Hay varias formas de realizar la transformación tf-idf, pero en pocas palabras, **tf-idf pretende representar el número de veces que una palabra dada aparece en un documento (una crítica de cine en nuestro caso) en relación con el número de documentos del corpus en los que aparece la palabra**.\n",
    "\n",
    "**Nota: Ahora que ya hemos hablado de los n-gramas, cuando hablo de \"palabras\" me refiero a cualquier n-grama (secuencia de palabras) si el modelo utiliza un n mayor que uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ln(N + 1 / count + 1) + 1\n",
    "Cuanto más común, menor es el TDFIDF. Cuanto más rara, mayor es el valor\n",
    "'''\n",
    "# Numero de documentos\n",
    "N = 3\n",
    "\n",
    "# Numero de veces que aparece\n",
    "count = 2\n",
    "\n",
    "1 + np.log((N + 1)/(count + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TfidfTransformer\n",
    "'''\n",
    "Cuanto más común, más bajo es el TfidfVectorizer\n",
    "'''\n",
    "sent1 = 'My name is Ralph'\n",
    "sent2 = 'Ralph is fat'\n",
    "sent3 = 'Ralph'\n",
    "\n",
    "test = TfidfVectorizer()\n",
    "test.fit_transform([sent1, sent2, sent3])\n",
    "print(test.idf_)\n",
    "print(test.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 + ln(N + 1 / count + 1)\n",
    "1 + np.log((3 + 1)/(3 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(reviews_train_clean)\n",
    "X = tfidf_vectorizer.transform(reviews_train_clean)\n",
    "X_test = tfidf_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "Recordemos que los clasificadores lineales tienden a funcionar bien en conjuntos de datos muy dispersos (como el que tenemos). Otro algoritmo que puede producir grandes resultados con un tiempo de entrenamiento rápido son las máquinas de vectores de soporte con un núcleo lineal.\n",
    "\n",
    "Construya un modelo con un rango de n-gramas de 1 a 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SVM con bigramas\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "\n",
    "\n",
    "def train_model_svm(X_TRAIN, X_TEST):\n",
    "    \n",
    "    svm = LinearSVC()\n",
    "    \n",
    "    params = {\n",
    "        'C': [0.01, 0.05, 0.25, 0.5, 1]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(svm, params, cv=5)\n",
    "    grid.fit(X_TRAIN, target)\n",
    "\n",
    "    print (\"Final Accuracy: %s\" % accuracy_score(target, grid.best_estimator_.predict(X_TEST)))\n",
    "    \n",
    "\n",
    "train_model_svm(X, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo final\n",
    "\n",
    "La eliminación de un pequeño conjunto de palabras vacías junto con un rango de n-gramas de 1 a 3 y un clasificador lineal de vectores de soporte muestra los mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   stop_words=stop_words)\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model_svm(X, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principales features positivas y negativas\n",
    "\n",
    "Obtener las características más importantes del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(reviews_train_clean)\n",
    "X = cv.transform(reviews_train_clean)\n",
    "\n",
    "log_reg = LogisticRegression(C=0.5)\n",
    "log_reg.fit(X, target)\n",
    "\n",
    "# Importancia de los coeficientes. En total, todas las palabras vectorizadas\n",
    "print(len(log_reg.coef_[0]))\n",
    "\n",
    "# Cada coeficiente va asociado a una palabra\n",
    "cv.get_feature_names()\n",
    "\n",
    "# Montamos un diccionario con palabra -> coeficiente\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), log_reg.coef_[0]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict(cv.transform(['This movie is horrible']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict(cv.transform(['This movie is incredible']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:5]:\n",
    "    print(best_positive)\n",
    "    \n",
    "print('################################')\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:5]:\n",
    "    print(best_negative)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec2a379ed5c25334a484232182c9d38ef8bd9861e2542d0c517568c4f99a9a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
