{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prostate cancer prediction \n",
    "\n",
    "#### Data\n",
    "\n",
    "We consider a medical study conducted on 97 men with prostate cancer.\n",
    "The focus is on the relationship between the prostate specific antigen (psa), which is elevated in men with prostate cancer, and others clinical measures. \n",
    "The others clinical measures are the predictors variables, gathered in a medical examination, and the amount of expression of the antigen associated with the cancer detection is the response variable (lpsa).\n",
    "\n",
    "Thus the data frame is made of 97 observations on 9 variables:\n",
    "* lcavol: log cancer volume\n",
    "* lweight: log prostate weight\n",
    "* age: patient age in years\n",
    "* lbph: log amount of benign prostatic hyperplasia\n",
    "* svi: seminal vesicle invasion\n",
    "* lcp: log of capsular penetration\n",
    "* gleason: Gleason score\n",
    "* pgg45: percent of Gleason score 4 or 5\n",
    "* lpsa: log prostate specific antigen\n",
    "\n",
    "The goal is to find models predicting the response lpsa.\n",
    "\n",
    "#### Models\n",
    "\n",
    "The data are represented by $n$ points in $p$ dimensions, thus the predictor variable is written $X\\in\\mathbb{R}^{n\\times p}$ and the response variable is $y\\in\\mathbb{R}^n$.\n",
    "\n",
    "In this work, we're insterested in the relationship between the predictor $X$ and the response $y$.\n",
    "To determine this relationship, we adopt regression models.\n",
    "The standard baseline is achieved with linear regression and we compare results for regularized regressions: **Ridge regression**, **Lasso** and **Elastic Net**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, metrics, model_selection\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpsa_data = pd.read_csv('data/prostate_dataset.txt', delimiter='\\t')\n",
    "lpsa_data = lpsa_data.loc[:, 'lcavol':]\n",
    "lpsa_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(lpsa_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lspa is almost normally distributed.\n",
    "* the presence of svi is binary\n",
    "* lcp: due to not proper measurements, for small values of capsular penetration, it has been arbitrarily set to -1.25.\n",
    "* gleason and pgg45 don't seem to be correlated...\n",
    "\n",
    "Let see the correlation between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE\n",
    "# sns.diverging_palette(145, 280, s=85, l=25, n=7)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(lpsa_data.corr(),\n",
    "           vmin=-0.1,\n",
    "           vmax=1,\n",
    "           cmap=sns.diverging_palette(145, 280, s=85, l=25, n=7),\n",
    "           annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The more correlated variable with the response lpsa is lcavol.\n",
    "  Thus in a data analysis, the lcavol variable must be included as a predictor.\n",
    "\n",
    "* The correlation matrix shows that gleason and pgg45 are actually correlated. \n",
    "  Indeed, the variable pgg45 measures the percentage of 4 or 5 Gleason scores that were recorded before the final current Gleason score.\n",
    "\n",
    "Let plot the relationship between the response lpsa and the lcavol feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lpsa_data['lcavol'], lpsa_data['lpsa'])\n",
    "plt.xlabel('lcavol', fontsize=16)\n",
    "plt.ylabel('lpsa', fontsize=16)\n",
    "plt.title(\"Relationship between lpsa and lcavol\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a pretty clear linear relationship with positive correlation, as seen on the correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpsa_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : the first rows \n",
    "# test : the last rows \n",
    "n_split = 60\n",
    "X_train = lpsa_data.iloc[:n_split,0:-2] # without pgg45\n",
    "X_test = lpsa_data.iloc[n_split:,0:-2]\n",
    "y_train = lpsa_data.iloc[:n_split,-1] # lpsa column\n",
    "y_test = lpsa_data.iloc[n_split:,-1]  # lpsa column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning models\n",
    "\n",
    "\n",
    "# 1) Linear regression baseline\n",
    "\n",
    "The linear regression attemps to model the relationship between the predictors variables $X$ and the response variable $y$.\n",
    "It consists in finding a linear function $f:\\mathbb{R}^p \\to \\mathbb{R}$ which predicts the response $y_i$ from the predictors $X_{i1},...,X_{ip}$ given $n$ observations for $i=1,...,n$.\n",
    "\n",
    "In Python, the linear regression is implemented as  [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) in the linear_model module of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "baseline_error = metrics.mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Train MSE:\", metrics.mean_squared_error(y_train, lr.predict(X_train)))\n",
    "print(\"Test MSE:\", metrics.mean_squared_error(y_test, lr.predict(X_test)))\n",
    "# print(\"Test RMSE:\", np.sqrt(metrics.mean_squared_error(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Regularization\n",
    "\n",
    "In order to avoid over-learning, the regularization method allows to control the model complexity.\n",
    "The model minimizes the error plus a regularization term $\\lambda Reg(\\beta)$ measuring the complexity, where $Reg(\\beta)$ is a penalty term and $\\lambda$ is an hyper-parameter.\n",
    "The hyper-parameter controls the relative influence of the error term and the amount of regularization.\n",
    "The optimal value of $\\lambda$ can be found by cross validation (see repository [cross-validation](https://github.com/christelle-git/cross-validation/)). \n",
    "\n",
    "## 2.1) Ridge regression \n",
    "\n",
    "In the Ridge regression, the regularization term is $Reg(\\beta) = ||\\beta||_2^2$.\n",
    "The Ridge regression allows to reduce the magnitude of the weights $\\beta_i$ of the linear regression, and thus avoid over-learning.\n",
    "The Ridge regression has a grouped selection effect: the correlated variables have the same weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgeR = Ridge(alpha = 10)\n",
    "ridgeR.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train MSE sin regularizaci贸n:\", round(metrics.mean_squared_error(y_train, lr.predict(X_train)),2))\n",
    "print(\"Test MSE sin regularizaci贸n:\", round(metrics.mean_squared_error(y_test, lr.predict(X_test)),2))\n",
    "\n",
    "print(\"Train MSE:\", round(metrics.mean_squared_error(y_train, ridgeR.predict(X_train)),2))\n",
    "print(\"Test MSE:\", round(metrics.mean_squared_error(y_test, ridgeR.predict(X_test)),2))\n",
    "# print(\"Test RMSE:\", np.sqrt(metrics.mean_squared_error(y_test, ridgeR.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 100\n",
    "alphas = np.logspace(-4, 3, n_alphas) \n",
    "\n",
    "coef_ridge = []\n",
    "err_ridge = []\n",
    "baseline = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    coef_ridge.append(ridge.coef_)\n",
    "    \n",
    "    y_pred = ridge.predict(X_test)\n",
    "    ridge_error = metrics.mean_squared_error(y_pred, y_test)\n",
    "    \n",
    "    err_ridge.append(ridge_error)\n",
    "    baseline.append(baseline_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(err_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, err_ridge, linewidth=5, color='red', label=\"Ridge regression\")\n",
    "ax.plot(alphas, baseline, linewidth=4,linestyle='--', color='blue', label='Linear regression')\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('error', fontsize=30)\n",
    "ax.legend(fontsize=30)\n",
    "plt.title(r'Regression error ($\\lambda$)', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge \n",
    "  \n",
    "# Train the model  \n",
    "ridgeR = Ridge(alpha = 10) \n",
    "ridgeR.fit(X_train, y_train) \n",
    "\n",
    "print(\"Train MSE: %0.4f\" % metrics.mean_squared_error(y_train, ridgeR.predict(X_train)))\n",
    "print(\"Test MSE: %0.4f\" % metrics.mean_squared_error(y_test, ridgeR.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The optimal value of the regularization coefficient $\\lambda$ is around 10.\n",
    "* For $\\lambda \\to 0$ the regularization terms vanishes leading to the same result as linear regression.\n",
    "* For $\\lambda \\to \\infty$ the regularization term magnitude dominates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coef_ridge, linewidth=5)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('weight value', fontsize=30)\n",
    "plt.title('Ridge coefficients paths', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ridge regression restricts somes variables by reducing their weights magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Least Absolute Shrinkage and Selection Operator\n",
    "\n",
    "The following method goes further by selecting some variables to be removed from the Ridge regression, thus reducing the dimension.\n",
    "The method is called Least Absolute Shrinkage and Selection Operator (Lasso) and the resulting simplified model is a **sparse model** or parsimonious model.\n",
    "In the Lasso, the regularization term is defined by $Reg(\\beta) = ||\\beta||_1$.\n",
    "\n",
    "The Lasso performs a model's feature selection: for correlated variables, it retains only one variable and sets other correlated variables to zero.\n",
    "The counterpart is that it obviously induces a loss of information resulting in lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lassoR = Lasso(alpha=10)\n",
    "lassoR.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train MSE sin regularizaci贸n:\", round(metrics.mean_squared_error(y_train, lr.predict(X_train)),2))\n",
    "print(\"Test MSE sin regularizaci贸n:\", round(metrics.mean_squared_error(y_test, lr.predict(X_test)),2))\n",
    "\n",
    "print(\"Train MSE: %0.4f\" % metrics.mean_squared_error(y_train, lassoR.predict(X_train)))\n",
    "print(\"Test MSE: %0.4f\" % metrics.mean_squared_error(y_test, lassoR.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(fit_intercept=False)\n",
    "\n",
    "coef_lasso = []\n",
    "err_lasso = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    coef_lasso.append(lasso.coef_)\n",
    "    y_pred = lasso.predict(X_test)\n",
    "    lasso_error = metrics.mean_squared_error(y_pred, y_test)    \n",
    "    err_lasso.append(lasso_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, err_lasso, linewidth=5, color='red', label=\"Lasso\")\n",
    "ax.plot(alphas, baseline, linewidth=4,linestyle='--', color='blue', label='Linear regression')\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('error', fontsize=30)\n",
    "ax.legend(fontsize=30)\n",
    "plt.title(r'Regression error ($\\lambda$)', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoR = Lasso(alpha = 0.02) \n",
    "lassoR.fit(X_train, y_train) \n",
    "\n",
    "print(\"Train MSE: %0.4f\" % metrics.mean_squared_error(y_train, lassoR.predict(X_train)))\n",
    "print(\"Test MSE: %0.4f\" % metrics.mean_squared_error(y_test, lassoR.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test MAPE: %0.4f\" % metrics.mean_absolute_percentage_error(y_test, lassoR.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The optimal value of the regularization coefficient $\\lambda$ is between $10^{-2}$ and $10^{-1}$.\n",
    "* For $\\lambda \\to 0$ the regularization terms vanishes so the Lasso regression tends to the linear regression.\n",
    "* For $\\lambda \\to \\infty$ the regularization term magnitude dominates the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coef_lasso, linewidth=5)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('weight value', fontsize=30)\n",
    "plt.title('Lasso coefficients paths', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Lasso removes somes variables by putting their weight to zero.<br>\n",
    "  This is the case if two variables are correlated.\n",
    "* As $\\lambda \\to \\infty$ weights vanish so the model becomes very **sparse**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Elastic net\n",
    "\n",
    "The Elastic Net method is a hybrid of the Ridge regression and the Lasso, thus overcomes the issue of losing information.\n",
    "The regularization term combines both the $L_1$ and the $L_2$ regularizations.\n",
    "More precisely, the regularisation term is set to $Reg(\\beta) = \\lambda((1-\\alpha)||\\beta||_1+\\alpha||\\beta||_2^2)$ where $\\alpha$ is an additional parameter to fit.\n",
    "\n",
    "The Elastic net has a selecting effect on variables as Lasso but keep correlated variables as Ridge regression.\n",
    "Thus the Elastic net model is less sparse than the Lasso, keeping more information. \n",
    "However the model is more demanding in computational resources.\n",
    "\n",
    "In what follows we present results for $\\alpha=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha = 1, l1_ratio = 0.5)\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train MSE: %0.4f\" % metrics.mean_squared_error(y_train, elastic_net.predict(X_train)))\n",
    "print(\"Test MSE: %0.4f\" % metrics.mean_squared_error(y_test, elastic_net.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_eln = []\n",
    "err_eln = []\n",
    "baseline = []\n",
    "for a in alphas:\n",
    "    elastic_net = ElasticNet(alpha = a, l1_ratio=1)\n",
    "    elastic_net.fit(X_train, y_train)\n",
    "    coef_eln.append(elastic_net.coef_)\n",
    "    y_pred = elastic_net.predict(X_test)\n",
    "    elasticnet_error = metrics.mean_squared_error(y_pred, y_test)\n",
    "    err_eln.append(elasticnet_error)\n",
    "    baseline.append(baseline_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(err_eln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, err_eln, linewidth=5, color='red', label=\"Elastic net\")\n",
    "ax.plot(alphas, baseline, linewidth=4,linestyle='--', color='blue', label='Linear regression')\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('error', fontsize=30)\n",
    "ax.legend(fontsize=30)\n",
    "plt.title(r'Regression error ($\\alpha=0.5$)', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(alpha = 0.025, l1_ratio=1)\n",
    "elastic_net.fit(X_train, y_train) \n",
    "\n",
    "print(\"Train MSE: %0.4f\" % metrics.mean_squared_error(y_train, elastic_net.predict(X_train)))\n",
    "print(\"Test MSE: %0.4f\" % metrics.mean_squared_error(y_test, elastic_net.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The optimal value of the regularization coefficient $\\lambda$ is between $10^{-2}$ and $10^{-1}$.\n",
    "* For $\\lambda \\to 0$ the regularization terms vanishes leading to the same result as linear regression.\n",
    "* For $\\lambda \\to \\infty$ the regularization term magnitude dominates the error which is smaller than with the Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coef_eln, linewidth=5)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('weight value', fontsize=30)\n",
    "plt.title(r'Elastic net coefficients paths ($\\alpha=0.5$)', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As expected the Elastic net keeps more variables than the Lasso.\n",
    "* Better performance can be obtained by varying the hyper-parameter $\\alpha$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "# Model selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear regression error:   %0.4f\" % baseline_error)    \n",
    "print(\"Minimun ridge error:       %0.4f\" % min(err_ridge))\n",
    "print(\"Minimum lasso error:       %0.4f\" % min(err_lasso))\n",
    "print(\"Minimum elastic net error: %0.4f\" % min(err_eln))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Lasso performs better than others methods (Elastic net: $\\alpha=0.5$). \n",
    "* The Lasso is more parsimonious but there is likely to be a loss of accuracy.\n",
    "* The Elastic net performs better than the Ridge regression (with $\\alpha=0.5$).\n",
    "* The Elastic net can be tuned to outperform Lasso but it is more demanding in computational resources.\n",
    "\n",
    "**=> The Elastic net is a good trade-off for accuracy and computational cost balance between the Ridge regression and the Lasso**.\n",
    "\n",
    "\n",
    "In order to optimize the model by fitting the optimal parameters, a cross validation can be performed.\n",
    "The functions sklearn.linear_model.RidgeCV, sklearn.linear_model.LassoCV and sklearn.linear_model.ElasticNetCV in Python perform an automatic tunning of hyperparameters for the Rigde regression, the Lasso and the Elastic Net respectively."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c77fdb427e7cbc9bc1367dd530fc2b36aacdbbde1ac83c85833b10dfa8b831c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
