{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones\n",
    "from collections import defaultdict\n",
    "\n",
    "# Las de siempre\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Aprendizaje por Refuerzo\n",
    "import gymnasium as gym  # https://gymnasium.farama.org/index.html\n",
    "\n",
    "# Otras librerías útiles\n",
    "# import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptos Básicos\n",
    "\n",
    "* Agente: Es la entidad que toma decisiones y realiza acciones en el entorno para lograr un objetivo.\n",
    "\n",
    "* Entorno: Es el contexto en el que el agente toma decisiones y realiza acciones. Puede ser cualquier sistema que el agente esté tratando de aprender y mejorar.\n",
    "\n",
    "* Episodio: Es un período completo de interacción entre el agente y el entorno, desde el inicio hasta que se alcanza un estado final.\n",
    "\n",
    "* Estado: Es una situación específica en la que se encuentra el entorno en un momento dado.\n",
    "\n",
    "* Recompensa: Es la retroalimentación que el agente recibe del entorno después de realizar una acción en un estado particular. Indica cuán buena o mala fue esa acción.\n",
    "\n",
    "* Política: Es la estrategia o conjunto de reglas que el agente sigue para tomar decisiones en diferentes estados.\n",
    "\n",
    "* Alpha (tasa de aprendizaje): Es cuánto el agente aprende de una nueva experiencia. Una tasa alta significa aprendizaje rápido, pero puede ser volátil.\n",
    "\n",
    "* Gamma (descuento): Controla cuánto valoramos las recompensas futuras. Un valor alto prioriza las recompensas a largo plazo.\n",
    "\n",
    "* Epsilon (exploración vs. explotación): Decide cuánto el agente elige acciones aleatorias en lugar de las aprendidas. A medida que disminuye, el agente tiende a elegir acciones que conoce mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q- Learning\n",
    "\n",
    "Q-Learning es un algoritmo de aprendizaje por refuerzo que permite a un agente aprender a tomar decisiones óptimas en un entorno desconocido. El agente mantiene una tabla (llamada tabla Q) que asigna un valor (Q-value) a cada par estado-acción. Estos valores representan la utilidad esperada de realizar una acción en un estado particular.\n",
    "\n",
    "Durante la interacción con el entorno, el agente actualiza los Q-valores usando la fórmula de actualización Q, que tiene en cuenta las recompensas inmediatas y los Q-values futuros.\n",
    "\n",
    "Con el tiempo, el agente aprende una política óptima al seleccionar las acciones con los Q-values más altos en cada estado. Q-Learning es efectivo para problemas de toma de decisiones secuenciales, como juegos o control de robots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q[state,action]=Q[state,action]+α⋅(reward+γ⋅max(Q[new_state,:])−Q[state,action])\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Del Punto A al Punto B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                        O----T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 6   # longitud del entorno\n",
    "actions = ['left', 'right']     # acciones disponibles\n",
    "epsilon = 0.9   # 'greedy policy': exploración vs explotación -> exploración\n",
    "alpha = 0.1     # 'learning rate': nivel de 'confianza' del agente -> desconfiado, cauto\n",
    "gamma = 0.9    # 'discount factor': si las recompensas disminuyen con el tiempo.\n",
    "max_episodes = 13   # número máximo de episodios\n",
    "fresh_time = 0.3    # tiempo de actualización entre episodios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # valores iniciales de la tabla Q\n",
    "        columns=actions,\n",
    "        )\n",
    "    # print(table)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_q_table(6, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_table):\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    if (np.random.uniform() > epsilon) or ((state_actions == 0).all()):  # non greedy, obligándolo a explorar\n",
    "        action_name = np.random.choice(actions)\n",
    "    else:   # greedy\n",
    "        action_name = state_actions.idxmax()    # escoger la acción con el valor más alto\n",
    "    return action_name\n",
    "\n",
    "\n",
    "def get_env_feedback(S, A):\n",
    "    # Interacción del agente con el entorno\n",
    "    if A == 'right':  # Si te mueves a la derecha\n",
    "        if S == n_states - 2:  # Y estabas en el penúltimo lugar (-1 es el último)\n",
    "            S_ = 'terminal'\n",
    "            R = 1  # recompensa\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # izquierda\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  # inicio\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R\n",
    "\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    env_list = ['-']*(n_states-1) + ['T']   # '---------T'\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode +1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(fresh_time)\n",
    "\n",
    "\n",
    "def rl():\n",
    "    # Aprendizaje por refuerzo (RL)\n",
    "    q_table = build_q_table(n_states, actions)\n",
    "    for episode in range(max_episodes):\n",
    "        step_counter = 0\n",
    "        S = 0\n",
    "        is_terminated = False\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "\n",
    "            A = choose_action(S, q_table)  # Elegir A basada en S actual y QT\n",
    "            S_, R = get_env_feedback(S, A)  # Obtener nuevo S y R\n",
    "            q_predict = q_table.loc[S, A]\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + gamma * q_table.iloc[S_, :].max() # Lo que tiene y lo que quiere\n",
    "                                                                 # Al principio, no sabe lo que quiere, será todo aleatorio\n",
    "            else:\n",
    "                q_target = R    \n",
    "                is_terminated = True\n",
    "\n",
    "            q_table.loc[S, A] += alpha * (q_target - q_predict) # Ajusta QT en base a sus \"experiencias\":\n",
    "                                                                # Si la realidad es mejor de lo que esperaba, ajusta sus expectativas hacia arriba\n",
    "                                                                # Si es peor, las ajusta hacia abajo.\n",
    "            S = S_ \n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "\n",
    "q_table = rl()\n",
    "print('\\r\\nQ-table:\\n')\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium (Gym)\n",
    "\n",
    "Gymnasium, anteriormente Gym, es una biblioteca de código abierto desarrollada por OpenAI que proporciona entornos de prueba estándar para experimentar con algoritmos de aprendizaje por refuerzo. Ofrece una variedad de entornos predefinidos, desde juegos simples hasta problemas complejos, permitiendo a los desarrolladores y científicos de datos entrenar y evaluar agentes de aprendizaje por refuerzo de manera consistente.\n",
    "\n",
    "Gym facilita la experimentación y comparación de diferentes enfoques de aprendizaje por refuerzo al proporcionar un marco unificado para la creación y evaluación de entornos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_keys = gym.envs.registry.keys()\n",
    "\n",
    "for key in env_keys:\n",
    "    print(key)\n",
    "\n",
    "print('------------------')\n",
    "print('Total de entornos:', len(env_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blackjack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://gymnasium.farama.org/_images/blackjack_AE_loop_dark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1',\n",
    "            #    nat = False # Otorga recompensa extra si el jugador empieza con 21.\n",
    "               sab = True, # Si el jugador tiene 21 y el dealer también, es un empate. http://www.incompleteideas.net/book/RLbook2020.pdf\n",
    "            #    render_mode='human'\n",
    "               )\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.env.blackjack import BlackjackEnv\n",
    "\n",
    "env = BlackjackEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed = 0)\n",
    "print(observation[0], ': Jugador')\n",
    "print(observation[1], ': Dealer')\n",
    "print(observation[2], ': Hay algún as \"utilizable\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegimos una acción aleatoria\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# 0 = stand (nos quedamos con el valor que tenemos)\n",
    "# 1 = hit (pedimos una carta más)\n",
    "\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(action, observation, terminated, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.agent.blackjack import BlackjackAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01 # alpha\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reducir la exploración con el tiempo\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(learning_rate = learning_rate,\n",
    "                       initial_epsilon = start_epsilon,\n",
    "                       epsilon_decay = epsilon_decay,\n",
    "                       final_epsilon = final_epsilon,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size = n_episodes)\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # actualizar al agente\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # actualizar si el entorno sigue funcionando\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación y Análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols = 3, figsize = (12, 5))\n",
    "axs[0].set_title(\"Recompensas por episodio\")\n",
    "# calcular y asignar un promedio de los datos para obtener un gráfico mejor\n",
    "reward_moving_average = (np.convolve(np.array(env.return_queue).flatten(),\n",
    "                                     np.ones(rolling_length),\n",
    "                                     mode = \"valid\") / rolling_length)\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "axs[1].set_title(\"Duración del episodio\")\n",
    "length_moving_average = (np.convolve(np.array(env.length_queue).flatten(),\n",
    "                                     np.ones(rolling_length),\n",
    "                                     mode = \"same\") / rolling_length)\n",
    "axs[1].plot(range(len(length_moving_average)),\n",
    "            length_moving_average)\n",
    "axs[2].set_title(\"Errores de estimación del agente\")\n",
    "training_error_moving_average = (np.convolve(np.array(agent.training_error),\n",
    "                                             np.ones(rolling_length),\n",
    "                                             mode = \"same\") / rolling_length)\n",
    "axs[2].plot(range(len(training_error_moving_average)),\n",
    "            training_error_moving_average)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grids(agent, usable_ace = False):\n",
    "    \"\"\"Create value and policy grid given an agent.\"\"\"\n",
    "    # convertir los valores Q en valores de estado (un promedio total)\n",
    "    # y crear un diccionario de políticas que sirve para relacionar observaciones con acciones\n",
    "    state_value = defaultdict(float)\n",
    "    policy = defaultdict(int)\n",
    "    for obs, action_values in agent.q_values.items():\n",
    "        state_value[obs] = float(np.max(action_values))\n",
    "        policy[obs] = int(np.argmax(action_values))\n",
    "\n",
    "    player_count, dealer_count = np.meshgrid(\n",
    "        # cartas del jugador y del dealer\n",
    "        np.arange(12, 22),\n",
    "        np.arange(1, 11))\n",
    "\n",
    "    # crear una tabla de valores\n",
    "    value = np.apply_along_axis(lambda obs: state_value[(obs[0], obs[1], usable_ace)],\n",
    "                                axis = 2,\n",
    "                                arr = np.dstack([player_count, dealer_count]))\n",
    "    value_grid = player_count, dealer_count, value\n",
    "\n",
    "    # crear una tabla de políticas\n",
    "    policy_grid = np.apply_along_axis(lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
    "                                      axis=2,\n",
    "                                      arr=np.dstack([player_count, dealer_count]))\n",
    "    return value_grid, policy_grid\n",
    "\n",
    "\n",
    "def create_plots(value_grid, policy_grid, title: str):\n",
    "    \"\"\"Creates a plot using a value and policy grid.\"\"\"\n",
    "    # el lado izquierdo son valores de estado y el derecho, políticas\n",
    "    player_count, dealer_count, value = value_grid\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # valores de estado\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "    ax1.plot_surface(player_count,\n",
    "                     dealer_count,\n",
    "                     value,\n",
    "                     rstride=1,\n",
    "                     cstride=1,\n",
    "                     cmap=\"viridis\",\n",
    "                     edgecolor=\"none\",)\n",
    "    plt.xticks(range(12, 22), range(12, 22))\n",
    "    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n",
    "    ax1.set_title(f\"Valores de estados: {title}\")\n",
    "    ax1.set_xlabel(\"Suma del jugador\")\n",
    "    ax1.set_ylabel(\"Carta del dealer\")\n",
    "    ax1.zaxis.set_rotate_label(False)\n",
    "    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n",
    "    ax1.view_init(20, 220)\n",
    "\n",
    "    # políticas\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n",
    "    ax2.set_title(f\"Política: {title}\")\n",
    "    ax2.set_xlabel(\"Suma del jugador\")\n",
    "    ax2.set_ylabel(\"Carta del dealer\")\n",
    "    ax2.set_xticklabels(range(12, 22))\n",
    "    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n",
    "\n",
    "    # leyenda\n",
    "    legend_elements = [Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n",
    "                       Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\")]\n",
    "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# valores de estado y políticas cuando hay as utilizable (as sirve como 11)\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=True)\n",
    "fig1 = create_plots(value_grid, policy_grid, title='Con As \"Utilizable\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores de estado y políticas cuando no hay as utilizable\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=False)\n",
    "fig2 = create_plots(value_grid, policy_grid, title='Sin As \"Utilizable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA (State-Action-Reward-State-Action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA es un algoritmo de aprendizaje por refuerzo que guía a un agente en un entorno interactivo. Ajusta sus estrategias basándose en experiencias reales, considerando las acciones tomadas, recompensas obtenidas y futuros estados previstos. Esto lo hace adecuado para entornos donde la cautela y la adaptación gradual son esenciales.\n",
    "\n",
    "Este algoritmo elige acciones basándose en alguna política actual y ajusta su comportamiento considerando las acciones futuras bajo esa política. En cambio, Q-Learning busca la acción que maximiza el valor futuro sin preocuparse por la política actual, tomando decisiones de manera más independiente y quizás más arriesgada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q[state,action]=Q[state,action]+α⋅(reward+γ⋅Q[new_state,new_action]−Q[state,action])\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "\n",
    "https://gymnasium.farama.org/environments/toy_text/frozen_lake/#action-space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=True, \n",
    "            #    render_mode = 'human',\n",
    "               # desc = sirve para hacer un mapa personalizado\n",
    "               )\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation # representa el estado (0 = estado inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info # representa la probabilidad de éxito de la acción, puesto que el hielo es resbaladizo. (al hacer reset siempre será 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0: Izquierda\n",
    "\n",
    "* 1: Abajo\n",
    "\n",
    "* 2: Derecha\n",
    "\n",
    "* 3: Arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation, reward, terminated, truncated, info = env.step(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "\n",
    "alpha = 0.9\n",
    "gamma = 0.9\n",
    "epsilon = 1\n",
    "epsilon_decay_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estados y Q-Table\n",
    "\n",
    "n_states = env.observation_space.n # n representa el número total de distintos estados\n",
    "n_actions = env.action_space.n\n",
    "Q = pd.DataFrame(np.zeros((n_states, n_actions)))\n",
    "Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Política \n",
    "\n",
    "def epsilon_greedy(Q, epsilon, n_actions, state):\n",
    "    if np.random.random() <= epsilon:           # Hay un 10 % de probabilidades de que el agente explore\n",
    "        return np.random.randint(n_actions)\n",
    "    else:\n",
    "        return np.argmax(Q[state, :])           # Del resto, siempre va a escoger el valor más alto de la tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(episodes, is_training=True, render=False, slippery=False):\n",
    "\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=slippery, render_mode='human' if render else None)\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.n\n",
    "\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    alpha = 0.9 \n",
    "    gamma = 0.9 \n",
    "    epsilon = 1 \n",
    "    epsilon_decay_rate = 0.0001 \n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for i in tqdm(range(episodes)):\n",
    "        state = env.reset()[0]  # states: 0 to 63, 0=top left corner,63=bottom right corner\n",
    "        action = epsilon_greedy(Q, epsilon, n_actions, state)\n",
    "        terminated = False      # True when fall in hole or reached goal\n",
    "        truncated = False       # True when actions > 200\n",
    "\n",
    "        while(not terminated and not truncated):\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            new_action = epsilon_greedy(Q, epsilon, n_actions, new_state)\n",
    "\n",
    "            if is_training:\n",
    "                Q[state, action] += alpha * (reward + gamma * Q[new_state, new_action] - Q[state, action])\n",
    "\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "\n",
    "        if reward == 1:\n",
    "            rewards_per_episode[i] = 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    sum_rewards = np.zeros(episodes)\n",
    "    for t in range(episodes):\n",
    "        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])\n",
    "    plt.plot(sum_rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(15, is_training=True, render=False, slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Cómo se guarda?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def run(episodes, is_training=True, render=False, slippery=False, save_path = None):\n",
    "\n",
    "    # . . .\n",
    "\n",
    "    if save_path is not None:\n",
    "            try:\n",
    "                with open(save_path, 'rb') as file:\n",
    "                    Q = pickle.load(file)\n",
    "                    print(f\"Agente cargado desde {save_path}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"No se encontró el archivo en la ruta {save_path}. Inicializando nueva tabla Q.\")\n",
    "\n",
    "    if Q is None:\n",
    "        Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    # . . .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
