{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"init\"></a>\n",
    "<h1>Machine Learning Guide</h1>\n",
    "\n",
    "En este notebook encontrarás una guía de cómo afrontar un problema de Machine Learning supervisado de clasificación o regresión. Está compuesto de explicaciones teóricas, ayudas en la toma de decisiones, código y enlaces de apoyo.\n",
    "\n",
    "<dl>\n",
    "  <dt><a href=\"#carga_datos\">1. Carga datos</a></dt>\n",
    "      <dd>Formato de los datos y cantidad de archivos</dd>\n",
    "    \n",
    "  <dt><a href=\"#reg_clas\">2. Problema Machine Learning</a></dt>\n",
    "      <dd>Clasificación o Regresión</dd>\n",
    "    \n",
    "  <dt><a href=\"#split_train_test\">3. Divide en train y test</a></dt>\n",
    "      <dd>Guardamos los datos de test desde el principio</dd>\n",
    "    \n",
    "  <dt><a href=\"#target\">4. Target</a></dt>\n",
    "      <dd>Distribución del target. ¿Desbalanceado?</dd>\n",
    "    \n",
    "  <dt><a href=\"#data_compr\">5. Comprensión de variables</a></dt>\n",
    "      <dd>Cómo son tus features</dd>\n",
    "    \n",
    "  <dt><a href=\"#feat_red_prelim\">6. Feat. Red. Preliminar</a></dt>\n",
    "      <dd>Reducción de features antes de empezar la analítica</dd>\n",
    "\n",
    "  <dt><a href=\"#univariant\">7. Análisis univariante</a></dt>  \n",
    "      <dd>Primeras impresiones de las variables. Distribuciones</dd>\n",
    "    \n",
    "  <dt><a href=\"#bivariant\">8. Análisis bivariante</a></dt>\n",
    "      <dd>Búsqueda de relaciones entre las variables</dd>\n",
    "  \n",
    "  <dt><a href=\"#del_features\">9. Eliminación de features</a></dt>\n",
    "      <dd>Features con muchos missings o alto grado de cardinalidad</dd>\n",
    "    \n",
    "  <dt><a href=\"#duplicates\">10. Duplicados</a></dt>\n",
    "      <dd>Comprobamos si el DataFrame tiene duplicados</dd>\n",
    "    \n",
    "  <dt><a href=\"#missings\">11. Missings</a></dt>\n",
    "      <dd>Tratamos los missings</dd>\n",
    "    \n",
    "  <dt><a href=\"#errors\">12. Anomalías y errores</a></dt>\n",
    "      <dd>Detección de datos incoherentes</dd>\n",
    "    \n",
    "  <dt><a href=\"#outliers\">13. Outliers</a></dt>\n",
    "      <dd>Tratamos los outliers</dd>\n",
    "    \n",
    "  <dt><a href=\"#feat_engi\">14. Feature Engineering</a></dt>\n",
    "      <dd>14.1 Transformaciones</dd>\n",
    "      <dd>14.2 Encodings</dd>\n",
    "      <dd>14.3 Nuevas Features</dd>\n",
    "      <dd>14.4 Escalados</dd>\n",
    "    \n",
    "  <dt><a href=\"#feat_reduc\">15. Feature Reduction</a></dt>\n",
    "      <dd>Filtrado de features por importancia</dd>\n",
    "    \n",
    "  <dt><a href=\"#choose_metric\">16. Escoger métrica del modelo</a></dt>\n",
    "      <dd>16.1 Métricas de clasificación</dd>\n",
    "      <dd>16.2 Métricas de regresión</dd>\n",
    "    \n",
    "  <dt><a href=\"#choose_models\">17. Decidir qué modelos</a></dt>\n",
    "      <dd>Factores que influyen en esta decisión</dd>\n",
    "    \n",
    "  <dt><a href=\"#hyperparmeters\">18. Elegir hiperparámetros</a></dt>\n",
    "      <dd>Según el volumen de datos y sus tipos</dd>\n",
    "    \n",
    "  <dt><a href=\"#pipelines\">19. Definimos pipelines y probamos</a></dt>\n",
    "      <dd>Dependerá de cada modelo. Ejecutamos</dd>\n",
    "    \n",
    "  <dt><a href=\"#results\">20. Resultados</a></dt>\n",
    "      <dd>Comprobamos si el error se ajusta al problema</dd>\n",
    "    \n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/ml_pipeline.png\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "iris_df = pd.DataFrame(load_iris()[\"data\"],columns=load_iris()[\"feature_names\"])\n",
    "iris_df[\"target\"] = load_iris()[\"target_names\"][load_iris()[\"target\"]]\n",
    "\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "diamonds_df = sns.load_dataset('diamonds')\n",
    "\n",
    "boston_df = pd.DataFrame(load_boston()[\"data\"],columns=load_boston()[\"feature_names\"])\n",
    "boston_df[\"target\"] = load_boston()[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"carga_datos\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 1. Carga datos\n",
    "\n",
    "#### Tipo de archivos\n",
    "Los más típicos [csv, txt, tsv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), [Excel (xlsx, xls)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html), [json](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html), [html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html) o [xml](https://pypi.org/project/pandas-read-xml/)\n",
    "\n",
    "Ten en cuenta que **el csv puede tener distintos separadores**, los datos del Excel pueden estar en una **hoja concreta**, y en una combinación de filas/columnas diferente al origen de la hoja. También podemos acceder a los archivos a través de **una URL**. Se usan las mismas funciones de pandas.\n",
    "\n",
    "Se recomienda abrir el csv en texto plano para ver el separador.\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"folder/file.csv\", sep=\";\")\n",
    "df = pd.read_csv(\"folder/file.csv\", sep=\"\\t\")\n",
    "df = pd.read_excel(\"folder/file.csv\", sheet_name=\"Sheet1\")\n",
    "df = pd.read_json(\"folder/file.json\")\n",
    "\n",
    "URL = \"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv\"\n",
    "df = pd.read_csv(URL)\n",
    "```\n",
    "\n",
    "#### Encoding de los datos\n",
    "\n",
    "<details>\n",
    "<summary>Descripción de encoding</summary>\n",
    "<p>\n",
    "    \n",
    "Los strings se almacenan internamente en un conjunto de bytes, caracter a caracter. Esta operación es lo que se conoce como *encoding*, mientras que pasar de bytes a string sería *decoding*. Bien, ¿y eso en qué nos afecta? Dependiendo del encoding, se suelen almacenar los caracteres en un espacio de bits de 0 a 255, es decir, en esa combinación de bits tienen que entrar todos los caracteres del lenguaje.\n",
    "\n",
    "El problema es que en toda esa combinación de bits no entran todos los caracteres del planeta, por lo que dependiendo del encoding que usemos, una combinación de bits significará una cosa u otra. Por ejemplo, una A mayuscula será lo mismo en el encodig europeo que en el americano, pero los bits reservados para representar una Ñ, en el encodig americano se traduce en otro caracter.\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "#### ¿Qué dificultades podemos encontrar?\n",
    "\n",
    "- **En la lectura tengamos un error de encoding**. Habrá que modificar el parámetro `encoding`, adaptándolo al archivo.\n",
    "\n",
    "- **El archivo ya se ha creado con el encodig erróneo**. Tendremos caracteres raros en algunas palabras. Un mapeo y sustitución podría valer para solventar el problema.\n",
    "\n",
    "- **Encontrar el encodig del archivo**. Es complicado saberlo antes de leer los datos. Aquí tenemos varios opciones:\n",
    "    - Leer con Python e ir probando los más habituales (`unicode`, `utf-8`, `latin1`, `ANSI`). Todas las [funciones de lectura de pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) tienen el tipo de encoding como argumento\n",
    "    - Abrir el archivo con el bloc de notas. El encoding viene en la esquina inferior derecha.\n",
    "    <details>\n",
    "    <summary>Ver imagen</summary>\n",
    "    <img src=\"./img/encoding.png\" alt=\"drawing\" width=\"400\"/>\n",
    "    </details>\n",
    "    - Usar la librería [`chardet`](https://pypi.org/project/chardet/) para saber el encoding adecuado del archivo.\n",
    "    \n",
    "\n",
    "\n",
    "#### ¿Cuántos DataFrames hay que cargar? train/test\n",
    "Simplemente, dos opciones:\n",
    "\n",
    "- **Todo el dataset junto**: necesitas un conjunto de test. Verás en el apartado <a href=\"#split_train_test\">donde se divide en train y test</a> cómo hacer esta división.\n",
    "\n",
    "- **Train/test por separado**: si tu conjunto de test no está etiquetado (tipico submission file de Kaggle), tendrás que proceder como en el punto anterior. En caso contrario ya tendrás definido tu conjunto de test, y el de train será el set de datos utilizados para el *cross validation* de los modelos.\n",
    "\n",
    "#### Join de datos\n",
    "No siempre tenemos los datos en un único dataframe, por lo que habrá que unir varios conjuntos de tal manera que haya una columna identificadora única para cada fila, con sus features y target asociados. **¿Cómo procedemos?**\n",
    "\n",
    "1. **Identifica las claves de cruce**: Cuando juntas dos tablas necesitas al menos una columna en común en ambas, por ejemplo un id de cliente. Para que si en una tabla tienes datos de pedidos y en otra datos personales del cliente, mediante su id, podrás juntar toda esa información en una misma tabla.\n",
    "2. **Escoge todas las columnas que vas a unir**: no siempre quieres quedarte con todos los datos de ambas tablas, por lo que habrá que elegir los datos a conservar de cada una.\n",
    "\n",
    "```Python\n",
    "left = df_pedidos[['id_cliente', 'pedido', 'descripcion']]\n",
    "right = df_cliente[['id_cliente', 'dirección', 'edad']]\n",
    "\n",
    "result = pd.merge(left, right, how='inner', on=['id_cliente'])\n",
    "```\n",
    "\n",
    "Describir cómo son los joins no es el objetivo de este notebook, pero te dejo [este enlace](https://realpython.com/pandas-merge-join-and-concat/) un buen artículo con varios ejemplos de joins.\n",
    "<details>\n",
    "<summary>Ver tipos de joins</summary>\n",
    "<img src=\"./img/joins.jpg\" alt=\"drawing\" width=\"500\"/>\n",
    "</details>\n",
    "\n",
    "#### ¿Me vale esta muestra para entrenar al modelo?\n",
    "Es muy sencillo cuando tenemos un dataset cerrado, que viene de un concurso de Kaggle, pero en un caso real eso se cumple pocas veces. Hay muchas bases de datos en la empresa, por no hablar de todos los sitios externos (web scraping o APIs) de donde podemos sacar datos. ¿Cómo sabemos que tenemos los datos suficientes para montar un modelo? Si vamos a obtener nuevos, ¿a qué fuentes acudo? ¿cómo es la calidad de estos datos?\n",
    "\n",
    "1. **Volumen**: Lo primero, necesitamos un buen volumen de datos. Menos de mil observaciones suele ser escaso para entrenar y testar un modelo de machine learning.\n",
    "2. **Calidad**: la calidad de los datos siempre que mejor que cantidad. Es mejor encontrar unas pocas features predictivas, cuyos datos sean fiables, que una gran cantidad de features que no aporten nada al modelo. Asegúrate de que los datos conseguidos son buenos y no están manipulados, ni modificados por otros integrantes de la empresa. Y de ser así, si vas a utilizarlos piensa que cuando realices predicciones, las entradas de tu modelo tendrán que ser esos mismos datos modificados. \n",
    "\n",
    "3. **Caso de uso**: piensa en el problema de negocio y planteate qué variables podrían ser predictivas, y si es factible conseguir esos datos.\n",
    "4. **Población**: asegúrante de que la población/muestra utilizada para entrenar se asemeje a la población con la que harás predicciones. Por ejemplo, si creas un modelo de tratamiento de imágenes, con el que predigas si unos pulmones tienen cáncer o no, tu modelo no tendrá un buen performance si lo entrenas con muestras de pulmones asiáticos y pretendes predecir muestras de pulmones caucásicos. O si entrenas solo con pulmones de mujeres e intentas predecir sobre pulmones de hombres.\n",
    "5. **Fuentes externas**: si tienes tiempo planteate acudir a fuentes externas a la empresa, mediante APIs, datasets de kaggle, páginas del gobierno... Por otro lado, no incluyas datos en el entrenamiento que luego no vayas a conseguir para las predicciones. Por ejemplo, si consigues una muestra concreta de datos que publicó una empresa, y ya no se van a publicar más, cuando vayas a hacer predicciones, no vas a poder contar con esos datos. Por último, piensa también que cuantas más fuentes externas, más dependencias tendrá tu modelo para realizar las predicciones y quizá no sea sencillo conseguir ese dato.\n",
    "\n",
    "6. **Crea tus propios datos**: ¿no tienes datos? \"*invéntatelos*\". Si necesitas crear un software de reconocimiento de imágenes para saber si alguien lleva gafas o no, saca fotos de amigos o familiares y utilizalas para el modelo. Otra opción es realizar encuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reg_clas\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 2. Problema Machine Learning\n",
    "Existen varios tipos de problemas supervisados de machine learning. Este notebook se centra en problemas de clasificación y regresión, sin series temporales ni RRNN.\n",
    "\n",
    "<img src=\"./img/ml_types.png\" alt=\"drawing\" width=\"650\"/>\n",
    "\n",
    "#### Algoritmos supervisados\n",
    "Tenemos los datos etiquetados\n",
    "1. **Clasificación**: el target del problema es un conjunto de valores discretos. Dos opciones:\n",
    "    - Binaria: paga/no paga, fuga de cliente/no fuga, contrae enfermedad/no contrae\n",
    "    - Multiclase: clasificar fruta (manzanas, peras, naranjas...), vinos (tinto, blanco, rosado)\n",
    "2. **Regresión**: el objetivo es predecir una variable continua como: distancia, temperatura, ingresos.\n",
    "\n",
    "#### Algoritmos no supervisados\n",
    "Los datos no están etiquetados. Es el propio algoritmo el que detecta patrones en los datos y los separa.\n",
    "1. **Clustering**: separación automática en grupos, como por ejemplo segmentación de clientes\n",
    "2. **PCA**: reducción de dimensionalidad\n",
    "\n",
    "Dentro de la clasificación anterior tendríamos los casos especiales de regresión con series temporales, o clasificación de imágenes/video con redes neuronales. **Este notebook se centra en clasificación y regresión de datos tabulares**.\n",
    "\n",
    "Es MUY IMPORTANTE conocer el tipo de problema de Machine Learning porque va a ser determinante para toda la analítica que viene a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split_train_test\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 3. Divide en train y test\n",
    "\n",
    "En todo problema de machine learning hay que reservar una porción de los datos para probar nuestros modelos. Es imposible saber a priori qué modelo y qué configuración es la que mejor se ajusta a nuestros datos, por lo que tendremos que probar varias combinaciones (entrenar con train) y después probar con test qué modelo da menos errores, y así elegir.\n",
    "\n",
    "<details>\n",
    "<summary><b>Cómo funciona el train_test_split</b></summary>\n",
    "<p>\n",
    "Los dos primeros argumentos son features y target. Dos cosas importantes. Lo primero, tienen que coincidir en dimensión, y lo segundo acuérdate de quitar el target en el conjunto de las features. Después especificamos la proporción de los datos que reservaremos para test (0-1).\n",
    "\n",
    "La división entre train y test la realiza mediante un muestreo aleatorio. Si tenemos un problema de clasificación binaria y los datos están ordenados de manera ascendente según el target, si simplemente nos quedamos con los primeros 70% para train y el resto, 30%, para test, caerán todos los 0s en train y todos los 1s en test. Por esta razón se realiza un muestreo aleatorio.\n",
    "    \n",
    "Por último, el argumento random_state. Como el muestreo es aleatorio, en cada ejecución tendríamos resultados diferentes. Fijando un número cualquiera conseguimos que el muestreo aleatorio que realiza siempre sea el mismo, de manera que si compartimos el notebook con otra persona o lo ejecutamos en otra ocasión, realizará el mismo split. Si queremos un split diferente simplemente habrá que poner otro número en el argumento random_state.\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "**¿Cuánto reservamos para test?** Entre un 15-30% es buena cifra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boston_df.drop('target', axis=1),\n",
    "                                                    boston_df['target'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realmente hay que dividir los datos en train, validation y test (ver imagen abajo). El conjunto de validation lo utilizamos para escoger modelo, y el de test para comprobar que nuestro modelo escogido generaliza bien. De momento vamos a preocuparnos de **guardar unos datos para test, y no los tocaremos hasta el final. No podemos contaminar train con los datos de test**. Esto supone seguir ciertas normas:\n",
    "\n",
    "1. **Scalers**: por ejemplo, si tenemos un StandardScaler en train, usar ese mismo StandardScaler en test (ver ejemplo abajo).\n",
    "2. **Outliers**: Me temo que los outliers no los voy a poder eliminar en test ya que en un problema real, podria entrar un outlier como input del modelo una vez hayamos hecho la puesta en producción. Cuidado con eliminar registros si estamos en una competición de Kaggle porque la cantidad de muestras que van a test no puede variar.\n",
    "3. **Missings**: si aplicábamos la media/mediana/moda en train, aplicar esa misma métrica en test. No apliques la media de test en train.\n",
    "4. **Feature reduction**: eliminar las features que quitábamos en train.\n",
    "5. **Feature engineering**: mismos cálculos que en train.\n",
    "\n",
    "**NOTA**: si se trata de un concurso de Kaggle, en el que ya te dan el conjunto de test aparte, no hace falta que dividas tu train para obtener un nuevo conjunto de test, sobre todo si tienes pocos datos. Utiliza todo el conjunto de train en el *cross validation* para escoger el mejor modelo, y después el test proporcionado por Kaggle para realizar una submission.\n",
    "\n",
    "<img src=\"./img/train_test_split.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de código para un StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Almaceno en el objeto scaler todo lo necesario para estandarizar, con los datos de train\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Utilizo los datos de train para escalar train y test.\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"target\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 4. Target\n",
    "Tenemos que echar un pequeño vistazo al target ya que hay ciertas casuísticas que no queremos tener, como por ejemplo un dataset desbalanceado o un target asimétrico. \n",
    "\n",
    "|Problema ML| Caso deseado | Caso no deseado |\n",
    "|---|----|----|\n",
    "|Clasificación| Dataset balanceado| Dataset desbalanceado|\n",
    "|Regresión| Distribución normal| Distribución asimétrica|\n",
    "\n",
    "#### Clasificación\n",
    "Si intentamos predecir si una persona va a padecer un cáncer o no, y entrenamos un modelo en el que el 97% de los casos no ha padecido cáncer, si el modelo predice siempre que no tiene cáncer, conseguirá un 97% de precisión. Y no es lo que estamos buscando. Queremos profundizar más en ese 3%, y que el modelo sepa discriminar bien entre una clase u otra. **Esto es lo que se conoce como dataset desbalanceado**. Lo ideal es que el dataset esté lo más equilibrado posible. ¿Cómo lidiar con ello?\n",
    "1. **Cambiando la métrica**: recall, f1-score o ROC curve (ver apartado de métricas).\n",
    "2. **Consiguiendo más datos**: Parece una tontería pero muchas veces ni nos lo planteamos\n",
    "3. **Resampling**: poner más copias del target con menor cantidad (over-sampling) o al revés (under-sampling), quitar datos del caso con mayor cantidad de datos. Under-sampling está bien cuando tenemos muchos datos, mientras que over-sampling lo tendremos que considerar cuando sean escasos. Prueba varios ratios.\n",
    "4. **Generar datos sintéticos**: existen funciones de sklearn que son capaces de generar nuevos datos, como SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "5. **Prueba nuevos algoritmos**: los árboles de decisión suelen funcionar bien en problemas desbalanceados.\n",
    "6. **Penalización en los modelos**: modelos que presten más atención a la clase minoritaria.\n",
    "7. **Weights**: Es posible aplicarle un peso diferente a cada clase, para que esté más balanceadas, para ello usamos el parámetro *class_weight*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "df = pd.read_csv(\"data/balance-scale.csv\")\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, y_rus = rus.fit_resample(df.loc[:, 'B':'1.3'], df['balance'])\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_ros, y_ros = ros.fit_resample(df.loc[:, 'B':'1.3'], df['balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# Original target\n",
    "sns.countplot(x=\"balance\", data=df, ax=axes[0])\n",
    "axes[0].set_title(\"Original target\")\n",
    "\n",
    "# RandomUnderSampler\n",
    "sns.countplot(y_rus, ax=axes[1])\n",
    "axes[1].set_title(\"RandomUnderSampler\")\n",
    "\n",
    "# RandomOverSampler\n",
    "sns.countplot(y_ros, ax=axes[2])\n",
    "axes[2].set_title(\"RandomOverSampler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algunas consideraciones antes de utilizar esta técnica**\n",
    "- Nunca testear sobre el conjunto sampleado. Siempre sobre el conjunto de test original \n",
    "- Si realizamos CrossValidation, siempre samplear DURANTE el CrossValidation, no antes. Es decir, meterlo en el Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión\n",
    "Lo mejor para los modelos es que tengamos todos los datos escalados y con una distribución normal. Algunos de los algoritmos asumen ese tipo de distribuciones en los datos como las regresiones lineales o las redes neuronales. Ya que esto rara vez ocurre, hay que transformar las variables. Para estandarizar una distribución podemos aplicar varios tipos de transformaciones <a href=\"#feat_engi\">(ver apartado transformaciones)</a>:\n",
    "\n",
    "1. Logaritmica\n",
    "2. Box-cox\n",
    "3. Cuadrado/cúbica\n",
    "\n",
    "Después puedes comprobar cuánto de normal es una variable mediante el **test de Saphiro** o un **Q-Q plot**.\n",
    "\n",
    "<details>\n",
    "<summary><b>Q-Q plot</b></summary>\n",
    "<p>\n",
    "Esta gráfica compara la variable en sí con respecto a una función de distribución gausiana estándar. Cuanto más se acerque a una línea recta, más estándar será la variable. Normalmente en el centro suele parecerse, pero se desvia por alguno de los lados. Eso quiere decir que tiene alguna asimetría.\n",
    "<br>\n",
    "\n",
    "<img src=\"./img/qqplot.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_target = load_boston()[\"target\"]\n",
    "sns.kdeplot(boston_target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "\n",
    "# Original target\n",
    "sns.distplot(boston_target, kde=False, ax=axes[0])\n",
    "axes[0].set_title(\"Original target\")\n",
    "\n",
    "# Logaritmic\n",
    "sns.distplot(np.log(boston_target),kde=False, ax=axes[1])\n",
    "axes[1].set_title(\"Log\")\n",
    "\n",
    "# Box-cox\n",
    "sns.distplot(stats.boxcox(boston_target)[0],kde=False, ax=axes[2])\n",
    "axes[2].set_title(\"Box-Cox\");\n",
    "\n",
    "# Power 2\n",
    "sns.distplot(np.power(boston_target, 2),kde=False, ax=axes[3])\n",
    "axes[3].set_title(\"Power 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA**: Después de aplicar una de estas transformaciones, habrá que volver a aplicar las transformaciones inversas sobre las predicciones. Si aplicamos logaritmo, luego irá `np.expm()`, box-cox con `scipy.special.inv_boxcox()` y la raíz enésima, en el caso de elevar a la potencia enésima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_compr\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 5. Comprensión de variables\n",
    "<img src=\"./img/dtypes.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Antes de ponernos a programar tenemos que entender muy bién cuál es la problemática y de qué datos disponemos, ya que este momento puede ser un punto de inflexión importante. Quizá con un pequeño análisis previo nos demos cuenta de que no hace falta un modelo de machine learning para solventar nuestro problema, o que no tenemos sufientes datos para realizar las predicciones planteadas inicialmente.\n",
    "\n",
    "\n",
    "Realiza una pequeña analítica de cada una de las variables. Esta analítica puede ir perfectamente en un Excel y nos va a ayudar a comprender la naturaleza de las variables y el sentido de las mismas respecto al target. Deberás recabar la siguiente información:\n",
    "1. **Variable**: nombre variable/alias\n",
    "2. **Data type**: cualitativa, cuantitativa, ordinal, continua...¿?\n",
    "3. **Segmento**: clasificar las variables según su significado. Si son variables demográficas, económicas, identificadores, tiempo...\n",
    "4. **Expectativas**: un pequeño indicador personal de si resultará útil la variable. ¿Necesito esta variable para la solución? ¿Cómo de importante será esta variable? ¿Esta info la recoge otra variable ya vista?\n",
    "5. **Conclusiones**: después del análisis anterior, llegar a unas conclusiones sobre la importancia de la variable.\n",
    "\n",
    "**¿Por qué hacemos esto?** Si nuestro problema tiene muchas features, realizar una selección \"a ojo\" puede resultar muy útil, siempre y cuando se haga con criterio de negocio. Además, podría darse el caso de empezar el EDA pre-modelo erróneamente sin darnos cuenta, y este pequeño análisis nos servirá para orientarnos ante discrepancias durante el análisis y posibles rectificaciones tempranas.\n",
    "\n",
    "Típicos métodos para echar un primer vistazo al dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estadísticos\n",
    "diamonds_df.describe()\n",
    "diamonds_df.describe(include='all')\n",
    "\n",
    "# Tipos de los datos\n",
    "diamonds_df.dtypes\n",
    "\n",
    "# Tipos de los datos y missings\n",
    "diamonds_df.info()\n",
    "\n",
    "# Columnas del dataset\n",
    "diamonds_df.columns\n",
    "\n",
    "# dimensiones del dataset\n",
    "print(\"Filas:\", diamonds_df.shape[0])\n",
    "print(\"Columnas:\", diamonds_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forzar el data type\n",
    "Fuerza los tipos de los datos. Si sabes que un float es un float, un str es un str, fuérzalo en el código para evitar errores posteriores. Hay que tener bien identificados los tipos de los datos.\n",
    "\n",
    "Por supuesto, probablemente haya missings en los datos. La conversión de estos missings da error, pero lo solventamos (de momento) con el argumento `errores='ignore'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import load_dataset\n",
    "df = load_dataset('titanic')\n",
    "\n",
    "# Diccionario con tipo en clave y valor columnas\n",
    "new_types = {\n",
    "    int: ['survived', 'age'],\n",
    "    float: ['fare'],\n",
    "    str: ['embarked', 'embark_town'],\n",
    "    bool: ['alone']\n",
    "}\n",
    "\n",
    "for key, value in new_types.items():\n",
    "    for i in value:\n",
    "        df[i] = df[i].astype(key, errors='ignore')\n",
    "\n",
    "# df['fare'] = df['fare'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reporte de variables\n",
    "Un pequeño reporte de las columnas, con sus tipos, % de missings y cardinalidad, resultará muy útil a lo largo de la analítica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_report(df):\n",
    "    # Sacamos los NOMBRES\n",
    "    cols = pd.DataFrame(df.columns.values, columns=[\"COL_N\"])\n",
    "\n",
    "    # Sacamos los TIPOS\n",
    "    types = pd.DataFrame(df.dtypes.values, columns=[\"DATA_TYPE\"])\n",
    "\n",
    "    # Sacamos los MISSINGS\n",
    "    percent_missing = round(df.isnull().sum() * 100 / len(df), 2)\n",
    "    percent_missing_df = pd.DataFrame(percent_missing.values, columns=[\"MISSINGS (%)\"])\n",
    "\n",
    "    # Sacamos los VALORES UNICOS\n",
    "    unicos = pd.DataFrame(df.nunique().values, columns=[\"UNIQUE_VALUES\"])\n",
    "    \n",
    "    percent_cardin = round(unicos['UNIQUE_VALUES']*100/len(df), 2)\n",
    "    percent_cardin_df = pd.DataFrame(percent_cardin.values, columns=[\"CARDIN (%)\"])\n",
    "\n",
    "    concatenado = pd.concat([cols, types, percent_missing_df, unicos, percent_cardin_df], axis=1, sort=False)\n",
    "    concatenado.set_index('COL_N', drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    return concatenado.T\n",
    "        \n",
    "data_report(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feat_red_prelim\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 6. Feat. Red. Preliminar\n",
    "En ocasiones es casi imposible llevar a cabo un análisis exploratorio si tienes una gran cantidad de features. Si ese es tu caso, deberías seguir alguna de las recomendaciones de este apartado.\n",
    "\n",
    "Básicamente lo que haremos es eliminar algunas features. ¿Qué criterios seguiremos?\n",
    "1. **Muchos missings**: a partir de un 40% de missings son features dificilmente salvables.\n",
    "2. **Features repetidas**: columnas repetidas. Asegúrate bien de que sea así.\n",
    "3. **Identificadores**: ids, nombres únicos. Son features que no aportan nada\n",
    "4. **Feature selection**\n",
    "\n",
    "    4.1 **Correlación lineal**: coeficientes de Pearson entre [-0.1, +0.1] respecto a nuestro target, suelen ser variables que no tiene nada que ver con el mismo.\n",
    "    \n",
    "    4.2 **Feature importance**: podemos tirar un modelo rápidamente que nos de la importancia de cada feature respecto al target: RandomForest se suele usar.\n",
    "    \n",
    "    4.3 **Varianza**: variables que no tienen varianza no nos interesan. El caso extremo sería una constante. No nos aporta nada. Es útil para ver que las variables pueden servir para el modelo pero no nos da una relación con el target.\n",
    "    \n",
    "    4.4 **Estadísticos**: la función de sklearn `SelectKBest` permite aplicar un test estadístico entre el target y cada uno de las features para determinar qué variables están relacionadas.\n",
    "\n",
    "En este momento lo que se pretende es eliminar features que sabes con certeza que son muy inútiles, por lo que los criterios para eliminar features en este punto del análisis serán mucho menos restrictivos que en posteriores (no queremos cargarnos información sin conocer bien el dataset). De momento, será suficiente con seguir los puntos 1, 2 y 3. El punto 4 se aplicarán si la cantidad de features es muy grande, por ejemplo más de 50 features.\n",
    "\n",
    "<details>\n",
    "<summary><b>¿Qué es la correlación lineal?</b></summary>\n",
    "<p>\n",
    "    \n",
    "La correlación, también conocida como coeficiente de correlación lineal (de Pearson), es una medida de regresión que pretende cuantificar el grado de variación conjunta entre dos variables.\n",
    "\n",
    "Por tanto, es una medida estadística que cuantifica la dependencia lineal entre dos variables, es decir, si se representan en un diagrama de dispersión los valores que toman dos variables, el coeficiente de correlación lineal señalará lo bien o lo mal que el conjunto de puntos representados se aproxima a una recta.\n",
    "    \n",
    "Se trata de un valor normalizado entre [-1, 1]. Cuanto más cerca de 1, mayor es la relación lineal creciente enter dos variables. Si nos acercamos a -1, tienen na relación lineal inversa fuerte (según aumenta una variable, disminuye otra), y cuanto más se acerca a 0, menor es la relación entre las variables.\n",
    "    \n",
    "<img src=\"./img/correlation.jpg\" alt=\"drawing\" width=\"600\"/>\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>¿Qué es el feature importance de un modelo?</b></summary>\n",
    "<p>\n",
    "Se trata de una serie de técnicas (depende de cada modelo) que asigna un score de importancia predictiva a las features de un modelo. Este score indica la importancia relativa de cada feature para la realización de las predicciones.\n",
    "    \n",
    "Estas técnicas son muy útiles por dos motivos:\n",
    "* Comprender qué variables han influenciado más en la predicción del target\n",
    "* Ver cuáles son las variables que tienen mayor relación con el target antes de correr cualquier modelo. Esto nos puede servir para hacer una primera preselección.\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "**Te recomiendo que utilices la función del apartado de comprensión de variables para elimar las que sean identificadores o tengan muchísimos missings**\n",
    "#### Muchos missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data with missings.csv')\n",
    "\n",
    "precent_missing = df.isnull().sum()*100/len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                'percent_missing': precent_missing}).sort_values('percent_missing', ascending=False)\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = missing_value_df[missing_value_df['percent_missing'] > 50].index.values\n",
    "print(\"Cols:\", cols_to_drop)\n",
    "\n",
    "print(\"Columnas pre drop:\", len(df.columns))\n",
    "\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(\"Columnas post drop:\", len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlación lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(load_boston()[\"data\"],columns=load_boston()[\"feature_names\"])\n",
    "df[\"target\"] = load_boston()[\"target\"]\n",
    "\n",
    "# Borra las columnas con una correlacion menor a 0.2\n",
    "corr = np.abs(df.corr()['target']).sort_values(ascending=True)\n",
    "print(corr)\n",
    "\n",
    "bad_corr_feat = corr[corr <0.2].index.values\n",
    "print(bad_corr_feat)\n",
    "\n",
    "df.drop(columns = bad_corr_feat, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import numpy as np\n",
    "#Load boston housing dataset as an example\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    "rf = RandomForestRegressor(n_estimators = 100)\n",
    "rf.fit(X, Y)\n",
    "\n",
    "scores = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), reverse=True)\n",
    "pd.DataFrame(scores, columns=['Score', 'Feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance estadístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "print(X.shape)\n",
    "sel = SelectKBest(k=5)\n",
    "X_new = sel.fit_transform(X, Y)\n",
    "print(X_new.shape)\n",
    "print(sel.scores_)\n",
    "\n",
    "pd.DataFrame({'column': names, 'score': sel.scores_}).sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"univariant\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 7. Análisis univariante\n",
    "Antes de meternos de lleno con los modelos, tenemos que realizar un análisis exploratorio para comprender qué datos tenemos entre manos.\n",
    "\n",
    "**¿Qué buscamos con este análisis?**\n",
    "1. Comprobar que las variables tienen sentido y sus unidades de medida también son correctas\n",
    "2. Detectar outliers\n",
    "3. Detección de errores en la variable\n",
    "4. Posibles features a eliminar\n",
    "5. Detectar distribuciones extrañas. Buscamos siempre distribuciones normales en los datos. Es como mejor trabajan los modelos. Si las distribuciones no son normales, habrá que transformarlas <a id=\"#feat_engi\">(ver apartado transformaciones)</a>.\n",
    "\n",
    "**¿Qué hacemos si tenemos muchas variables en el dataset?** Si es el caso, deberíamos utilizar una matriz de correlación o correr algún modelo para que nos de un indicador de qué features serán las más predictivas. Podemos empezar por esas features. Otra manera de decidir qué features escoger es mediante un análisis de negocio. Si vas a predecir si un paciente padece una enfermedad, o la temperatura que hará mañana, o la probabilidad de fuga de un cliente en una telco, piensa qué variables necesitarías en un modelo de ese estilo, y cuáles de las que disponen cumplen con esos criterios.\n",
    "\n",
    "<details>\n",
    "<summary><b>Tipos de gráficas</b></summary>\n",
    "<p>\n",
    "    \n",
    "Para representar una sola variable se utilizan las siguientes opciones:\n",
    "\n",
    "    1. Box plot: está bien para ver la dispersión y outliers, pero no representa con exactitud la distribución.\n",
    "\n",
    "    2. Función de densidad: funciona bien para ver la forma de su distribución, su asimetría e incluso outliers, pero el eje no está en unidades de la variable, sino en probabilidades.\n",
    "\n",
    "    3. Histograma: Parecido a la función de densidad. En este caso cada barra nos da un conteo. Muy útil. Se suele combinar con la función de densidad.\n",
    "\n",
    "    4. Violin plot: Parecido a la caja, pero en este caso representa también la distribución.\n",
    "    \n",
    "En resumen, los que más se suelen utilizar son los histogramas y diagramas de caja.\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Gráficas en función de tus datos</b></summary>\n",
    "<p>\n",
    "    \n",
    "La representación individual de la variable resulta muy útil ya que nos da información de outliers, asimetría y dispersión. Pero como el objetivo es predecir el target, relacionar la variable con el target siempre aportará información enriquecedora ya que podremos ver relaciones entre cada variable vs el target. Por ello, llevaremos a cabo diferentes representaciones en función de los dos tipos de problemas planteados en este notebook:\n",
    "\n",
    "    1. Clasificación, feature numérica: podemos representar la feature en un diagrama de densidad (o histograma), en varios colores, dependiendo de la categoría del target.\n",
    "\n",
    "    2. Clasificación, feature categórica: representa en un diagrama de barras un conteo de valores. Puedes añadirle un color a cada barra en función del target.\n",
    "\n",
    "    3. Regresión, feature numérica: un scatter plot de ambas variables.\n",
    "\n",
    "</p>\n",
    "</details> \n",
    "\n",
    "#### Clasificación, feature numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "# Funcion de densidad\n",
    "sns.distplot(iris_df['sepal length (cm)'], hist = False, ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Función de densidad\")\n",
    "\n",
    "# Histograma\n",
    "sns.distplot(iris_df['sepal length (cm)'],\n",
    "             kde=False,\n",
    "             color='slategray',\n",
    "             ax=axes[0, 1]);\n",
    "\n",
    "axes[0, 1].set_title(\"Histograma\")\n",
    "\n",
    "# Funcion de densidad + histograma\n",
    "sns.distplot(iris_df['sepal length (cm)'],\n",
    "            kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},\n",
    "            hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\n",
    "                      \"alpha\": 1, \"color\": \"g\"},\n",
    "             ax=axes[0, 2])\n",
    "\n",
    "\n",
    "axes[0, 2].set_title(\"Funcion de densidad + hist.\")\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(iris_df['sepal length (cm)'], color=\"slategray\", ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Box plot\")\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(iris_df['sepal length (cm)'], color=\"slategray\", ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Violin plot\")\n",
    "\n",
    "\n",
    "# Funcion de densidad + target\n",
    "sns.kdeplot(data=iris_df, x='sepal length (cm)', hue = 'target', ax=axes[1, 2])\n",
    "axes[1, 2].set_title(\"FDP + target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Clasificación feature categórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titanic_df[\"survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Conteo de categorica\n",
    "sns.countplot(titanic_df, x = titanic_df[\"embark_town\"],ax=axes[0])\n",
    "axes[0].set_title(\"Variable categórica\")\n",
    "\n",
    "# Categorica vs target\n",
    "sns.countplot(titanic_df, x = titanic_df[\"embark_town\"], hue=titanic_df['survived'], ax=axes[1], dodge=True)\n",
    "axes[1].set_title(\"Categórica vs target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión (numérica y categórica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target de precio en el dataset de diamonds\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Numeric target\n",
    "sns.distplot(diamonds_df['price'], ax = axes[0])\n",
    "axes[0].set_title(\"Numeric target\")\n",
    "\n",
    "# Numeric vs Numeric\n",
    "sns.scatterplot(x = diamonds_df['price'],y = diamonds_df['carat'],  ax = axes[1])\n",
    "axes[1].set_title(\"Numeric vs Numeric\")\n",
    "\n",
    "# Categorica vs target\n",
    "sns.boxplot(data=diamonds_df, x='cut', y='price', ax=axes[2])\n",
    "axes[2].set_title(\"Numeric vs categoric\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Código para análisis rápido univariante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "cols = 5\n",
    "rows = int(np.ceil(float(iris_df.shape[1]) / cols))\n",
    "for i, column in enumerate(iris_df.columns):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    ax.set_title(column)\n",
    "    if iris_df.dtypes[column] == np.object:\n",
    "        iris_df[column].value_counts().plot(kind=\"bar\", axes=ax)\n",
    "    else:\n",
    "        iris_df[column].hist(axes=ax)\n",
    "        plt.xticks(rotation=\"vertical\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bivariant\"></a>  \n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "## 8. Análisis bivariante\n",
    "Utilizamos este tipo de análisis para ver cómo son las relaciones entre nuestros datos, dos a dos. Los objetivos de este análisis son:\n",
    "1. **Relación con el target**: ver qué tipo de relación hay entre cada feature con el target. Con el tipo de relación intuiremos el modelo que mejor le vendrá a nuestros datos.\n",
    "2. **Eliminar algunas features**: comprobar si hay features con una alta correlación lineal entre ellas. Eso significa dos cosas:\n",
    "    - Si hay dos features que se parecen mucho, sobra una. ¿Cuál? La que tenga mas missings o menor correlación con el target\n",
    "    - Si dos features tienen una correlación lineal alta podemos sufrir de multicolinearidad. Te dejo [este artículo](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/#:~:text=Multicollinearity%20occurs%20when%20independent%20variables,model%20and%20interpret%20the%20results.) para más información.\n",
    "\n",
    "Podemos llevar a cabo dos tipos de anális bivariante, uno mediante visualizaciones, y otro con tests y medidas estadísticas. Las visualizaciones serían el método más artesanal de ver esas relaciones entre los datos, mientras que con las medidas estadísticas puedo establecer un criterio algo más automatizado.\n",
    "\n",
    "#### ¿Qué visualizaciones puedo hacer?\n",
    "Lo mejor es hacer un grid de scatterplots. Suena muy rimbombante, pero en seaborn no es más que una línea de código. Resulta útil si tenemos todo variables numéricas, pero para categóricas no nos vale, tendremos que acudir a tablas y diagramas de barras stacked o agrupadas.\n",
    "\n",
    "#### ¿Qué medidas estadísticas puedo utilizar?\n",
    "Lo habitual es utilizar el coeficiente de pearson (lo que te da una matriz de correlación) para cada dos variables, ya que es un valor normalizado entre [-1, 1]. Si el valor está en torno a 0, las dos variables tienen muy poca correlación lineal, y cuanto más cercano a 1 o -1 mejor será su correlación lineal directa o inversa respectivamente. A partir de +/- 0.6 o 0.7 suele ser una buena correlación. No obstante un coeficiente de pearson alto [no siempre asegura una buena relación entre los datos](https://www.tylervigen.com/spurious-correlations)\n",
    "\n",
    "El problema viene cuando queremos ver relaciones entre variables categóricas. En este caso ya no es tan útil el coeficiente de pearson. **Para detectar este tipo de relaciones recomiendo utilizar el coeficiente de Phik o Cramér's V**. En la [documentación de pandas profiling](https://github.com/pandas-profiling/pandas-profiling) tienes algunos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numericas vs target categórico\n",
    "sns.pairplot(iris_df,\n",
    "            kind='scatter',\n",
    "            hue='target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categórica vs categorica\n",
    "pd.crosstab(titanic_df['survived'],\n",
    "           titanic_df['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupby(['embarked', 'survived']).size().reset_index().pivot(columns='embarked', index='survived', values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = titanic_df.groupby(['embarked', 'survived']).size().reset_index().pivot(columns='embarked', index='survived', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix/heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establece los límites de colores entre [-1, 1], así como un rango de colores de oscuro a oscuro, pasando por claro (0 correlación)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(diamonds_df.corr(),\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            center=0,\n",
    "            cmap=sns.diverging_palette(145, 280, s=85, l=25, n=10),\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            linewidths=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phik matrix/heatmap\n",
    "Esta matriz es un muy buen indicador sobre la relación entre los datos, teniendo en cuenta variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install phik\n",
    "import phik\n",
    "phik_matrix = diamonds_df.phik_matrix()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(phik_matrix,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            center=0,\n",
    "            cmap=sns.diverging_palette(145, 280, s=85, l=25, n=10),\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            linewidths=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"del_features\"></a>     \n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "## 9. Eliminación de features\n",
    "Una vez hecho un primer análisis de nuestras variables, llega el momento de limpiar el dataset. Lo primero que nos planteamos es si sobran features, en función de toda la información ya recabada. Parecido al primer apartado de eliminación de features, pero en este caso no seremos tan restrictivos, ya que eliminaremos features con más información de la que disponíamos antes.\n",
    "* **Features con valores constantes**.\n",
    "* **Features con alto porcentaje de missings**: En general más de un 20%-30% de missings en una sola feature es bastante, por lo que podría considerarse la eliminación de la misma.\n",
    "* **Features con identificadores**. Todo lo que sean ids y nombres tienen poco poder predictivo. Podría darse el caso de que un id correle mucho con una fecha. Sería el caso de pedidos de una empresa, cuyo id va aumentando según pasan los días, pero igualmente suele aportar poco.\n",
    "* **Features de strings largos**: Se podría aplicar algún tratamiento de texto con expresiones regulares o incluso NLP (Natural Language Processing), pero en general suelen eliminarse este tipo de features.\n",
    "* **Features categóricas con alta cardinalidad**: viene a ser parecido a los identificadores. Se suelen eliminar.\n",
    "* **Correlación o Phik matrix**: podríamos aplicar estos indicadores para eliminar algunas features. Recuerda que la matriz de correlción sólo te da información sobre la relación LINEAL entre los datos numéricos.\n",
    "\n",
    "Estas decisiones no se toman tampoco a la ligera y dependerá mucho de la cantidad de features que tengamos disponible. Si es poca, andaremos con más cuidado a la hora de eliminar información, mientras que si son muchas las features no habrá problema en eliminar algunas tipo identificador o que contengan gran cantidad de missings.\n",
    "\n",
    "```Python\n",
    "# Elimina features pero no sustituye\n",
    "df.drop(columns=['feature1', 'feature2'])\n",
    "\n",
    "# Elimina features y sustituye en el dataframe\n",
    "df.drop(columns=['feature1', 'feature2'], inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con este código podrás eliminar de manera automática features con mucha cardinalidad o gran cantidad de missings\n",
    "\n",
    "# Max (%) cardinalidad que permitiremos en una feature\n",
    "cardi = 20\n",
    "\n",
    "# Max (%) de missings que permitiremos en una feature\n",
    "max_miss = 30\n",
    "\n",
    "def drop_cols(df_, max_cardi=20, max_miss=30):\n",
    "    df = df_.copy()\n",
    "    delete_col = []\n",
    "    \n",
    "    for i in df.columns:\n",
    "        missings = df[i].isnull().sum() * 100 / len(df)\n",
    "        \n",
    "        # Elimina por missings\n",
    "        if missings >= max_miss:\n",
    "            df.drop(i, 1, inplace=True)\n",
    "            continue\n",
    "        \n",
    "        # Elimina por cardinalidad en variables categoricas\n",
    "        if df[i].dtype.name in ('category', 'object'):\n",
    "            if df[i].nunique()*100/len(df) >= max_cardi:\n",
    "                df.drop(i, 1, inplace=True)          \n",
    "        \n",
    "    return df\n",
    "\n",
    "drop_cols(titanic_df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"duplicates\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "## 10. Duplicados\n",
    "Datos duplicados en general no aportan nada, a no ser que hayamos remuestrado el target, en el caso de tener un dataset desbalanceado <a href=\"#target\">(ver apartado de análisis del target)</a>. Por tanto, en general se suelen eliminar.\n",
    "\n",
    "**¿Qué es un duplicado?** Pregunta obvia, pero en ocasiones no está tan claro. ¿Sabrías identificar cuál es tu clave única en los datos? Un identificador único puede ser los clientes, o los pedidos hechos en tu tienda, o tus pacientes. Pero si tu clave son tus clientes, podrías tener varios clientes \"duplicados\" porque dispones de datos concretos de ese cliente a fecha 14 y 15 de marzo. O tienes datos únicos de clientes, o tienes datos únicos por cliente y fecha, las dos cosas no puedes.\n",
    "\n",
    "**¿Cómo elimino los duplicados?** Lo más fácil es un `drop_duplicates()` de pandas, que eliminará los registros duplicados teniendo en cuenta TODAS las columnas. Pero hay ocasiones en las que quiero eliminar registros duplicados en función de unas pocas columnas (ver ejemplo abajo).\n",
    "\n",
    "**NOTA**: cuidado con eliminar datos en test si estamos en una competición de Kaggle. A la muestra de test no le podemos quitar registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Clientes': ['Alba', 'Carolina', 'Alberto', 'Alberto'],\n",
    "                   'Fecha': ['2020-01','2020-01','2020-01','2020-02'],\n",
    "                   '¿Compró?': [1, 1, 1, 1]})\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alberto en principio está duplicado. Pero si aplico un drop duplicates no me lo elimina ya que TODAS las filas son únicas. Para ello habrá que decirle a pandas qué columna/as queremos que utilice para discrimiar si un registro es un duplicado o no. En este caso será únicamente el nombre. [Aqui tienes la documentación del drop_duplicates()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len original\", len(df1))\n",
    "\n",
    "print(\"len drop_duplicates():\", len(df1.drop_duplicates()))\n",
    "\n",
    "de_new = df1.drop_duplicates(subset = 'Clientes', keep = 'last')\n",
    "print(\"len drop_duplicates() por cliente:\", len(de_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"missings\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "## 11. Missings\n",
    "Suponen un problema ya que la mayoría de los modelos no saben tratar los missings y por tanto tendremos que inferir sus valores. Por aclarar, un missing no es un 0, ni un string vacío, ni un False. Es un hueco en los datos, el vacío, nos falta un dato en una de las features. Eso es un missing.\n",
    "\n",
    "#### ¿Por qué aparecen missings?\n",
    "* **Missings en la extracción de los datos**: estos se suelen identificar rápido, por ejemplo, que no se ha leido bien un CSV, y todos los valores se los tome como uno solo.\n",
    "* **Missings completamente aleatorios**: la probabilidad de que haya un valor missing es la misma para todas las observaciones.\n",
    "* **Missings aleatorios**: aleatoriamente hay missings en una columna. Por ejemplo, datos de edades de una pagina web, vemos que hay más missings de mujeres que de hombres.\n",
    "* **Missings que dependen de los inputs**: Por ejmplo, en un estudio médico, si un diagnóstico es dudoso, hay una alta probabilidad de ser descartado. Cuando estos missings se les pone a \"Dudoso\", dejan de ser valores aleatorios.\n",
    "* **Missings que dependen de su propio valor**: por ejemplo, ingresos de las personas, los que tienen mucho o muy poco, se tenderá a no poner el valor.\n",
    "* **Missings en tratamiento de datos**: pueden aparecer missings debido a una incorrecta lectura o escritura, tratamiento de los datos y merge con otras tablas.\n",
    "\n",
    "#### Métodos para tratar los missings\n",
    "* **Imputación sobre caso similar**: tengo en cuenta otra variable, por ejemplo si tengo la altura de alumnos de una clase, podria calcular los missings teniendo en cuenta si son chicos o chicas. O si hay missings de densidad de población, pero tengo la población y el área, podría recalcularlo. Esta opción es lo ideal, razonando los missings con los propios datos.\n",
    "* **Borrado**: Aquí hay dos opciones: si hay un missing en una columna, nos cargamos toda la fila. El borrado se realiza cuando tenemos pocos missings y muchas observaciones. De lo contrario, eliminaríamos muchos datos.\n",
    "* **Imputación por valor concreto**: No es lo más habitual, pero puede ocurrir que sepamos de antemano por qué hay missings en esa feature. Quizá todos los días hay problemas en la carga de la base de datos con el carácter Ñ, y cuando debería aparecer España, en realidad está en missing. Este método también se utiliza cuando hay missings en la variable categórica, le ponemos un nombre genérico y equivaldría a añadir una categoría nueva a la feature.\n",
    "* **Imputación de Media/Mediana/Moda**: es el método más frecuente, sobretodo si tenemos poco tiempo. Para variables numéricas imputamos normalmente la media, aunque si la variable tiene asimetría, suele dar mejores resultados la mediana. En el caso de que sea una variable categórica lo más habitual es utilizar la moda.\n",
    "* **Imputacion + flag**: es igual que el caso anterior, pero además añadimos una nueva columna binaria por cada feature con missings imputados para indicarle al modelo que en ese lugar había un missing.\n",
    "* **Modelo**: es lo más sofisticado. Dividimos el dataset en dos: uno con missings y el otro sin, que se usa para entrenar al modelo. Este método se suele comportar bien, a no ser que no tengan mucha relación los datos. Lo habitual es usar KNN, regresión logística o lineal. El KNN rellena los missings teniendo en cuenta el resto de variables del dataset, buscando similitudes con registros que tengan las variables parecidas. Se puede usar para cualitativas y cuantitativas, las variables con muchos missings se pueden tratar fácilmente. Lo malo es que es muy lento y la elección del k-value es crítica.\n",
    "\n",
    "Vamos al código para imputar missings. Lo primero que necesitas saber es los missings que tienes en los datos. Para ello lo mejor es utilizar la función que hemos creado en el apartado de comprensión de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_report(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En los ejemplos con drop NO estamos sobreescribiendo el dataset\n",
    "\n",
    "# Para eliminar una columna, si tiene muchos missings\n",
    "iris_df.drop(columns = ['sepal length (cm)'])\n",
    "\n",
    "# Eliminamos las filas si encuentra missing en cualquier columna del dataset\n",
    "iris_df.dropna()\n",
    "\n",
    "# Elimina las filas donde todos sus elementos sean missing\n",
    "iris_df.dropna(how='all')\n",
    "\n",
    "# Elimina filas si encuentra missings en las siguientes columnas\n",
    "iris_df.dropna(subset=['sepal length (cm)', 'sepal width (cm)'])\n",
    "\n",
    "# Imputar con un valor\n",
    "iris_df['sepal length (cm)'] = iris_df['sepal length (cm)'].fillna(0)\n",
    "\n",
    "# Imputar con la media o mediana\n",
    "iris_df['sepal length (cm)'] = iris_df['sepal length (cm)'].fillna(iris_df['sepal length (cm)'].mean())\n",
    "iris_df['sepal length (cm)'] = iris_df['sepal length (cm)'].fillna(iris_df['sepal length (cm)'].median())\n",
    "\n",
    "# Interpolar. Imputa missings en función del valor anterior y el siguiente. Tendrá sentido en series temporales, es decir, con datos ordenados\n",
    "iris_df['sepal length (cm)'] = iris_df['sepal length (cm)'].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputacion mediante KNN\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputer.fit(titanic_df[['pclass', 'age', 'sibsp', 'parch', 'fare']])\n",
    "titanic_df[['pclass', 'age', 'sibsp', 'parch', 'fare']] = imputer.transform(titanic_df[['pclass', 'age', 'sibsp', 'parch', 'fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_report(titanic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"errors\"></a> \n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "## 12. Anomalías y errores\n",
    "También podemos encontrarnos casos extraños en los datos que no siempre se corresponden con un outlier. Un outlier es un valor atípico en una variable, pero que puede tener cierto sentido. Sin embargo, un error es un valor que no tiene nada que ver con la variable, como que aparezcan edades en negativo, o haya ciertos datos a 99999. También podría ocurrirnos en texto, un texto mal decodificado, o unas fechas erróneas. Posibles errores que habría que corregir:\n",
    "1. **Numeros positivos que aparecen como negativos**: variables numéricas que siempre deberían tener numeros positivos y en algún caso (o en muchos) presente datos negativos, habrá que tratarlo como si fuesen missings.\n",
    "2. **Datos de fecha incorrectos**: asegúrate que estás leyendo bien la fecha en el formato americano o europeo. Suele cambiarse la posición del día por la del mes. Comprueba que los años de las fechas son coherentes, ya que una lectura errónea puede llevar a fechas del año 9999. El método que no te va a fallar (pero tambiéne s el más costoso) es importar la fecha en string, y luego parsear cada uno de sus elementos mediante la función `split()`.\n",
    "3. **Encoding del texto**: si ves caracteres raros, sospecha del encoding del texto. Abre el archivo de los datos en texto plano y comprueba que no tenga caracteres raros. Prueba varios encodings en la lectura. Los más habituales: ascii, ansi, utf-8, latin1. Tienes más información en <a href=\"#carga_datos\">el apartado de carga de datos</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"outliers\"></a>   \n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "## 13. Outliers\n",
    "#### ¿Qué es un outlier?\n",
    "Se trata de un valor atípico dentro de nuestros datos. Un valor que se desvía mucho de las métricas estadísticas de centralidad (media, moda, mediana). En el análisis exploratorio de datos suele ser algo a estudiar, el por qué tengo algunos valores atípicos en los datos. No obstante, en machine learning los outliers en los datos implican penalizaciones en los modelos, sobretodo los que trabajan con distancias y con Gradient Descent. Por tanto, hay que lidiar con ellos.\n",
    "\n",
    "#### ¿Cómo detecto los outliers?\n",
    "Lo mejor es gráficamente, aunque también puedes parametrizar una serie de umbrales en tus features, a partir de los cuales consideras que son outliers. Hay dos maneras de verlos gráficamente, mediante análisis:\n",
    "1. **Univariante**\n",
    "2. **Bivariante**\n",
    "3. **Medidas estadísticas**: cuánto me desvío de la media, o cuánto de lejos estoy del IQR.\n",
    "\n",
    "#### ¿Qué gráficas utilizo para visualizar los outliers?\n",
    "Lo mejor son boxplots, histogramas, diagramas de densidad, scatter plots y count plots para categóricas.\n",
    "\n",
    "#### ¿Qué técnicas hay para detectar outliers?\n",
    "1. **Gráficamente**: Datos que se desvien mucho.\n",
    "2. **Cuartiles**: se ve en un diagrama de caja. Datos que caigan fuera del rango +/- 1.5*IQR. Este 1.5 es muy restrictivo por lo que se suelen probar valores del 3 al 5. Dependerá mucho de cada feature.\n",
    "3. **Desviación estándar**: todo lo que caiga fuera de (media +/- N*std) de la variable. Normalmente N es un valor de 3 a 5.\n",
    "\n",
    "Ten en cuenta siempre el contexto de negocio para considerar los outliers, no el puramente numérico. El propio negocio del dataset que estés analizando es el que define qué es un outlier y qué no.\n",
    "\n",
    "Detectar gráficamente es la técnica más habitual, pero si quieres hacerlo de una manera más automatizada puedes probar con las otras dos opciones. Además ten en cuenta que **los outliers siempre dependen del negocio** por lo que deberás analizarlos bien antes de eliminarlos.\n",
    "\n",
    "<img src=\"./img/boxplot.png\" alt=\"drawing\" width=\"600\"/>    \n",
    "\n",
    "<details>\n",
    "<summary><b>Causas de los outliers</b></summary>\n",
    "<p>\n",
    "    \n",
    "* **Entrada de datos**: errores humanos causados durante la recolección, como por ejemplo escribir 10.000 en vez de 1.000. Se comprueba fácil que esto es un outlier, comparándolo con los otros valores.\n",
    "* **Errores de medida**: es el más común. Se producen cuando para los datos se han utilizado instrumentos de medida. Si tenemos 10 maquinas para medir el peso, es posible que una de ellas esté averiada.\n",
    "* **Errores experimentales**: se han medido mal los datos en el propio experimento, no por el aparato de medida. Si hay 7 corredores esprintando y uno ha salido más tarde, su tiempo será mayor, pero realmente su tiempo de carrera no refleja este dato.\n",
    "* **Errores intencionados**: suele relacionarse con reportes que incluyan datos personales.\n",
    "* **Error en el procesado de datos**: en lo que cargamos los datos de varias fuentes, es posible que hayamos introducido algún error en el procesamiento de los datos.\n",
    "* **Error en el muestreo**: se hayan introducido muestras de otro tipo en el dataset. Por ejemplo, estamos midiendo la altura de los alumnos de futbol, y se nos han colado algunos de basket.\n",
    "* **\"Errores\" naturales**: cuando no es artificial. Es posible que sean así los datos.\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "#### ¿Qué hago con los outliers?\n",
    "Tenemos varias opciones:\n",
    "1. **Eliminarlos**: es la técnica más habitual y sencilla\n",
    "2. **No hacer nada**: si no son exagerados. Los árboles de decisión y SVM (en este orden) son robustos frente a outliers.\n",
    "3. **Transformaciones logarítmicas**: elimina asimetría en las features, y por tanto reduce el efecto de los outliers. Para más info<a href=\"#feat_engi\"> ver el apartado de transformaciones.</a>\n",
    "4. **Binning**: discretiza la variable en varios grupos. Esto me va a permitir incluir los outliers en un grupo donde haya otros datos no considerados como outiers (ver ejemplo abajo)\n",
    "5. **Imputación**: igual que con los missings, sustituir los valores. Esto tendrá sentido si los outliers están bien analizados, y desde el punto de vista de negocio conviene sustituirlos por un valor concreto.\n",
    "6. **Tratamiento por separado**: si es un número significativo de outliers quizá merezca la pena separar los datos y tratarlos como otro modelo aparte.\n",
    "\n",
    "#### ¿Qué modelos son más sensibles a outliers?\n",
    "Los modelos a los que MENOS afectan los outliers son los árboles de decisión, por lo que todo lo que sean árboles y derivados tendrán un comportamiento robusto frente a outliers. Los SVM también funciónan bien. A los algoritmos que más les afectan los outliers son a los basados en distancia (KNN) o Gradient Descent (regresiones).\n",
    "\n",
    "**NOTA** Mucho cuidado al eliminar outliers en X_train. Hay que eliminar los mismos registro en el target (y_train). Para ello lo mejor es juntarlos, eliminarlos y cuando vayas a modelar separar X de y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target de precio en el dataset de diamonds\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "\n",
    "# Outliers con boxplot\n",
    "sns.boxplot(diamonds_df['depth'], ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Outliers con boxplot\")\n",
    "\n",
    "# Feature susceptible de transformación\n",
    "sns.boxplot(diamonds_df['price'], ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"Feature susceptible de transformación\")\n",
    "\n",
    "# Transformacion logaritmica\n",
    "sns.boxplot(np.log(diamonds_df['price']), ax=axes[0, 2])\n",
    "axes[0, 2].set_title(\"Transformacion logaritmica\")\n",
    "\n",
    "# Posibles outliers mediante FDP\n",
    "sns.kdeplot(diamonds_df['price'], ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Posibles outliers en FDP\")\n",
    "\n",
    "# Outliers mediante bivariante\n",
    "sns.scatterplot(data=diamonds_df, x='depth', y='price', ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Outliers mediante bivariante\")\n",
    "\n",
    "# Transformacion de FDP\n",
    "sns.kdeplot(np.log(diamonds_df[\"price\"]), ax=axes[1, 2])\n",
    "axes[1, 2].set_title(\"Transformacion de FDP\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "def outliers_quantie(df, feature, param=1.5):  \n",
    "        \n",
    "    iqr_ = iqr(df[feature], nan_policy='omit')\n",
    "    q1 = np.nanpercentile(df[feature], 25)\n",
    "    q3 = np.nanpercentile(df[feature], 75)\n",
    "    \n",
    "    th1 = q1 - iqr_*param\n",
    "    th2 = q3 + iqr_*param\n",
    "    \n",
    "    return df[(df[feature] >= th1) & (df[feature] <= th2)].reset_index(drop=True)\n",
    "\n",
    "diamonds_df2 = outliers_quantie(diamonds_df, 'depth', 7)\n",
    "print(\"Len original:\", len(diamonds_df))\n",
    "print(\"Len sin outliers en depth:\", len(diamonds_df2))\n",
    "sns.scatterplot(data=diamonds_df2, x='depth', y='price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_meanSd(df, feature, param=3):   \n",
    "    media = df[feature].mean()\n",
    "    desEst = df[feature].std()\n",
    "    \n",
    "    th1 = media - desEst*param\n",
    "    th2 = media + desEst*param\n",
    "\n",
    "    return df[((df[feature] >= th1) & (df[feature] <= th2))  | (df[feature].isnull())].reset_index(drop=True)\n",
    "\n",
    "\n",
    "diamonds_df2 = outlier_meanSd(diamonds_df, 'depth', 4)\n",
    "print(\"Len original:\", len(diamonds_df))\n",
    "print(\"Len sin outliers en depth:\", len(diamonds_df2))\n",
    "sns.scatterplot(data=diamonds_df2, x='depth', y='price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tecnicas de binning\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "\n",
    "sns.kdeplot(diamonds_df[\"price\"], ax=axes[0])\n",
    "axes[0].set_title(\"Variable original\")\n",
    "\n",
    "sns.histplot(diamonds_df[\"price\"], bins=3, ax=axes[1])\n",
    "axes[1].set_title(\"Binning\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(diamonds_df[\"price\"], bins=3, density=True)\n",
    "bin_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feat_engi\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 14. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1 Transformaciones\n",
    "Como ya se ha mencionado varias veces a lo largo del notebook, lo ideal es tener distribuciones normales en todas las variables (target incluido) de un modelo. Por desgracia no siempre es así, y variables como el salario de una población suelen tener una larga cola hacia la derecha, es decir, presentan asimetría positiva.\n",
    "\n",
    "#### ¿Cómo solucionamos esto?\n",
    "Aplicando transformaciones logarítimicas, cuadradas o cúbicas. Las logarítimicas suelen dar mejor resultado.\n",
    "\n",
    "Para comprobar si tenemos una distribución normal, lo mejor es graficarla, pero siempre tenemos la opción de calcular un valor numérico que nos indique si la distribución es normal o no. Para ello podemos aplicar el test de Shapiro, cuya hipótesis nula es que la variable sigue una distribución normal, mientras que la alternativa sostiene que no sigue una distribución normal. Si el p-value es mucho menor que su nivel de significación (0.05), tendremos que rechazar la hipotésis nula y asegurar que la variable NO sigue una distribución normal.\n",
    "\n",
    "Otra forma de visualizar la normalidad de una variable es mediante un Q-Q plot\n",
    "<details>\n",
    "<summary><b>Q-Q plot</b></summary>\n",
    "<p>\n",
    "Esta gráfica compara la variable en sí con respecto a una función de distribución gausiana simétrica. Cuanto más se acerque a una línea recta, más normal será la variable. Normalmente en el centro suele parecerse, pero se desvia por alguno de los lados. Eso quiere decir que tiene alguna asimetría.\n",
    "<br>\n",
    "\n",
    "<img src=\"./img/qqplot.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "from sklearn.datasets import load_boston\n",
    "from scipy.stats import skew\n",
    "\n",
    "boston_target = load_boston()[\"target\"]\n",
    "\n",
    "# Muy por debajo del nivel de significación (0.05) no se considera distribución normal.\n",
    "print(\"Saphiro:\", shapiro(boston_target).pvalue)\n",
    "\n",
    "# Para comprobar la asimetría de una variable siempre podemos calcular su valor skew\n",
    "# 0 si es simetrica, >0 cola hacia la derecha, <0 cola hacia la izquierda\n",
    "print(\"Asimetria:\", skew(boston_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(boston_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# Original target\n",
    "print(\"p-value Shapiro test Original: \", shapiro(boston_target).pvalue)\n",
    "sns.distplot(boston_target, kde=False, ax=axes[0])\n",
    "axes[0].set_title(\"Original target\")\n",
    "print(\"Asimetria:\", skew(boston_target))\n",
    "\n",
    "# Logaritmic\n",
    "print(\"p-value Shapiro test Logaritmic: \", shapiro(np.log(boston_target),).pvalue)\n",
    "sns.distplot(np.log(boston_target),kde=False, ax=axes[1])\n",
    "axes[1].set_title(\"Log\" + 'np-value Shapiro ' +\"shapiro_test\")\n",
    "print(\"Asimetria:\", skew(np.log(boston_target)))\n",
    "\n",
    "# Box-cox\n",
    "print(\"p-value Shapiro test Box-cox: \", shapiro(stats.boxcox(boston_target)[0]).pvalue)\n",
    "sns.distplot(stats.boxcox(boston_target)[0],kde=False, ax=axes[2])\n",
    "axes[2].set_title(\"Box-Cox\");\n",
    "print(\"Asimetria:\", skew(stats.boxcox(boston_target)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 Encodings\n",
    "¿En qué dataset no vamos a encontrar variables cualitativas? Variables categóricas que se componen de un conjunto de valores finito, pequeño, y normalmente en formato string. Aportan muchísima información, pero necesitamos transformarlas a valores numéricos para que los modelos puedan trabajar con ellas, ya que no admiten texto.\n",
    "\n",
    "Una primera aproximación podría ser asignar un número del 1 a n a cada categoría, siendo n la cantidad de valores únicos de esa variable. Esto no es del todo correcto ya que si tuviésemos países en una variable y asignamos 1 a China, 2 a Marruecos y 3 a Brasil, estamos diciendo que China está a 2 de distancia de Brasil y que Marruecos es un país a medio camino entre China y Brasil. Estamos cometiendo un error y es que le estamos asignando un orden a variables que originalmente no tienen orden.\n",
    "\n",
    "Por tanto, estas son las preguntas que deberías plantearte cuando vayas a codificar variables categóricas:\n",
    "\n",
    "**¿La variable es binaria?** En este caso nos dará igual el orden. Asignamos 1 y 0 indistintamente y listo.\n",
    "\n",
    "**¿La variable está ordenada?** Si estuviese ordenada, nuestra primera aproximación sería perfectamente válida. Eso sí, aplicando el mapeo correcto. Si tenemos calificaciones tipo ['Baja', 'Media', 'Alta', 'Muy Alta'], asegúrante de codificarlas en el orden correcto: [0, 1, 2, 3].\n",
    "\n",
    "**¿Y si la variable no está ordenada?** Aplicaremos un dummy encoder o OneHotEncoder. Básicamente consiste en crear una columna nueva con cada feature. Todas las columnas a 0, excepto cuando aparezca esa categoría concreta. En ese caso pondremos un 1.\n",
    "<details>\n",
    "<summary><b>OneHotEncoder</b></summary>\n",
    "<p>\n",
    "\n",
    "<img src=\"./img/onehotencoder.jpg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Para más detalle sobre la diferencia entre un OneHot o dummy encoder visita [este enlace](https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn#:~:text=One%2Dhot%20encoding%20converts%20it,up%20with%20kn%2Dk%20variables.)\n",
    "\n",
    "**¿Y si tenemos una gran cardinalidad en la variable?** No es muy viable aplicar un OneHot o dummy encoder, ya que crearía demasiadas features nuevas. Otra posible opción es emplear un HashingEncoder. Establecemos un número m de columnas, y traduce mediante una función hash todas las categorias de la feature en solo las m columnas, por supuesto ya no van a ser binarias. En [este enlace](https://medium.com/flutter-community/dealing-with-categorical-features-with-high-cardinality-feature-hashing-7c406ff867cb#:~:text=Feature%20hashing%20maps%20each%20category,within%20a%20pre%2Ddetermined%20range.&text=Convert%20data%20into%20a%20vector,or%20%E2%80%9Cthe%20hashing%20trick%E2%80%9D.) tienes información sobre cómo funciona.\n",
    "\n",
    "**Consejos para trabajar con train y test**. Recuerda no contaminar train con los datos de test. Esto quiere decir que si usas un OneHotEncoder sobre una categoria de colores, donde en test hay colores que no tienes en train, NO crees columnas dummy de esos colores en train porque en teoría train no sabe que existen esos colores de test. Crea los dummies en train con los datos de train, y utiliza la misma codificación para test. Si alguna de las categorias de train no coincide con las de test y viceversa, simplemente aparecerá todo el registro a 0s.\n",
    "\n",
    "| Método | Descripción |\n",
    "|--------|-------------|\n",
    "|DummyEncoding/OneHotEncoder|Sustituye la variable por 1s y 0s en varias columnas <br />Por sencillez, se recomienda usar la función de dummies de pandas, en vez del OneHotEncoder de sklearn|\n",
    "|LabelEncoding|Sustituye cada categoria por un valor numérico creciente|\n",
    "|Count encoding|Sustituye cada categoría por la frecuencia de aparición de la misma|\n",
    "|Mean encoding|Para problemas de regresión. Sustituye cada categoría por la media del target para esa categoría|\n",
    "|Hashing|Utilizado cuando tenemos una alta cardinalidad en los datos|\n",
    "\n",
    "Los métodos mean encoding o el count encoding no son tan efectivos como el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = pd.DataFrame({'City': ['SF', 'SF', 'NYC', 'NYC', 'Seattle', 'Seattle'],\n",
    "                   'Rent': [3999, 4000, 3499, 3500, 2499, 2500]})\n",
    "\n",
    "dummy_df = pd.get_dummies(df, prefix=['city'])\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si queremos aplicar un mapeo personalizado\n",
    "# Para encodig binario se recomienda aplicar un mapeo, para no equivocarnos.\n",
    "def mapping(x):\n",
    "    if x == 'SF':\n",
    "        return 1\n",
    "    elif x == 'NYC':\n",
    "        return 2\n",
    "    elif x == 'Seattle':\n",
    "        return 3\n",
    "    else:\n",
    "        return 9999\n",
    "\n",
    "df['Custom map'] = df['City'].apply(mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos una alta cardinalidad en los datos, utilizamos un hasher\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "df = pd.DataFrame({'CA': ['Madrid', 'Cataluña', 'Andalucia', 'Pais Vasco', 'Andalucia', 'Madrid', 'Valencia', 'Andalucia']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = FeatureHasher(n_features=3, input_type='string')\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h.transform(df['CA'])\n",
    "f.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3 Nuevas features\n",
    "Llega la parte más creativa de la analítica, en la que intentamos sacarle jugo a los datos. El objetivo de este apartado es crear nuevas features a partir de las que ya tenemos. Muy habitual cuando no tenemos muchas features y hay que buscar nuevas.\n",
    "\n",
    "Algunas técnicas que podemos aplicar:\n",
    "1. **Si tenemos fechas y horas**, podemos obtener nuevos datos como:\n",
    "    - Año, trimestre, mes, dia, hora, minuto, segundo.\n",
    "    - Un indicador de si es de dia o de noche. O incluso si es por la mañana, horario laboral...\n",
    "    - Otro indicador de si es un dia de fin de semana o laboral.\n",
    "    - Decadas o épocas.\n",
    "2. **Concatenar campos**: si tenemos variables categóricas, puede resultar interesante realizar otras agrupaciones con otras categóricas. Por ejemplo, si tenemos ventas por cliente, segmentadas por sexo y tipo de producto. Una nueva feature puede ser combinar ambos, para tener un indicador agregado de tipo de producto y sexo.\n",
    "3. **Agrupar**: sería el binning que ya hemos visto. Por ejemplo agrupar edades en varios bins.\n",
    "4. **Suma/Resta/Multiplicación...**: los ratios suelen ser útiles. Cuidado con las combinaciones lineales entre variables, sobretodo si estamos con regresión lineal.\n",
    "5. **Agregaciones de otros datasets**:  por ejemplo, si estamos viendo pedidos de clientes, sus datos agregados de lo que suelen gastar, o la media de pedidos que hacen al mes, se podrían añadir al dataset.\n",
    "6. **Datos espaciales**: si tuviésemos varias localizaciones geográficas, podriamos calcular por ejemplo distancias.\n",
    "7. **Texto**: podemos sacar info de ciertas palabras clave. Por ejemplo, si tenemos nombres con Mr, Mis, Mrs... es posible extraer la variable sexo de manera sencilla, empleando únicamente el nombre.\n",
    "8. **Datos externos macro**: si estamos trabajando con paises o poblaciones, podemos agregar nuevos datos al dataset como el PIB, cantidad de población, distribución por sexo...acudiendo a fuentes como el INE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4 Escalados\n",
    "Dependiendo del modelo que vayamos a utilizar, resultará útil escalar las features. **¿Qué significa escalar una variable?**\n",
    "Consiste en cambiar el orden de magnitud de la variable, conservando su distribución. Estas transformaciones se aplican por separado a cada feature. Se suele calcular unas métricas como media, desviación estándar, y mediante una fórmula aplicamos la transformación a cada valor de la feature.\n",
    "\n",
    "#### ¿Por qué realizamos transformaciones?\n",
    "Hay ciertas características que le vienen bien a los modelos. Tener todos los datos en la misma escala es una de ellas. Los modelos trabajan mejor y entrenan más rápido cuando la escala de todos los datos es la misma. Esto lo vamos a conseguir con un StandardScaler o un MinMaxScaler. Otra de las características que deberían tener los modelos es tener datos con distribuciones gausianas. Esto no siempre es así ya que muchas veces las distribuciones son bimodales o asimétricas. Para solucionar esto aplicamos logaritmos, elevamos al cuadrado o al cubo (veremos en el siguiente apartado).\n",
    "\n",
    "#### StandardScaler\n",
    "\n",
    "<details>\n",
    "<summary><b>Explicación del StandardScaler</b></summary>\n",
    "<p>\n",
    "La estandarización (o Z-Score) consiste en reescalar una feature para conseguir las propiedades de una distribucion normal estándar: $\\mu=0$ and $\\sigma = 1$.\n",
    "\n",
    "Donde $\\mu$ es la media y $\\sigma$ la desviación estándar. De esta forma cada valor de la feature se transforma mediante la fórmula:\n",
    "\n",
    "\\begin{equation} z = \\frac{x - \\mu}{\\sigma}\n",
    "\\end{equation} \n",
    "\n",
    "No solo conseguimos que todas las variables se encuentren en la misma escala, sino que beneficia al proceso de entrenamiento, particularmente todos los algoritmos que dependan de distancias (como KNN) o los que utilicen Gradient Descent, consiguiendo una convergencia del algoritmo más rápida.\n",
    "\n",
    "**¿En qué algoritmos usaremos StandardScaler?** Este escalado va a venir bien en todos menos en los árboles, a los que no les afecta el escalado. Algoritmos de clustering también van a demandar un StandardScaler ya que buscan similitudes entre features, midiendo la distancia entre las mismas. PCA también.\n",
    "\n",
    "**¿Debo estandarizar el target?** Lo primero, tendría que ser un problema de regresión. Aunque no se suele estandarizar. Se deja como está y las predicciones van en la escala del target. No obstante, se podría realizar dicha estandarización y después aplicarle la inversa (ver código abajo).\n",
    "\n",
    "**¿Cómo aplico el escalado a train y test?**. Primero creo los objetos scaler con los datos de train (SOLO los de train), y aplica ese scaler tanto a train como a test. Así no contamino el conjunto de train con los datos de test.\n",
    "    \n",
    "**¿Cambia la forma de la distribución con este scaler?** NO. Si tenías una distribución asimétrica, seguirá siendo asimétrica pero con otra escala.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Cargamos datos\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42) \n",
    "print(X_train[0])\n",
    "\n",
    "# Creo el scaler con los datos de train\n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X_train) # Lo \"entreno\". Calculo su media y std para cada feature\n",
    "X_train = scal.transform(X_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "print(X_train[0])\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X_test = scal.transform(X_test)\n",
    "\n",
    "# Si quiero recuperar la anterior escala\n",
    "X_train = scal.inverse_transform(X_train)\n",
    "print(X_train[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMaxScaler\n",
    "\n",
    "<details>\n",
    "<summary><b>Explicación del MinMaxScaler</b></summary>\n",
    "<p>\n",
    "MinMax scaler o normalización. Muy similar al StandardScaler, aunque en este caso dejamos los datos acotados entre [0, 1], donde 1 corresponde al máximo de la variable y 0 se asignará al mínimo de la variable. Para aplicar la transformación a cada feature tendremos que aplicar esta fórmula:\n",
    "    \n",
    "\\begin{equation}\n",
    "X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}}\n",
    "\\end{equation}\n",
    "\n",
    "Este escalado es muy habitual en tratamiento de imágenes, donde los colores de pixeles vienen acotados (normalmente entre 0 y 255). Esa escala es demasiado grande y perjudica mucho el tiempo de entrenamiento. Escalando el valor de cada pixel entre 0 y 1 resultará en una mejora en los tiempos de entrenamiento.\n",
    "    \n",
    "**¿En qué algoritmos usaremos MinMaxScaler?** Seguro en imágenes. Y en el resto (menos árboles, a los que no les afecta el escalado), al igual que el StandardScaler, suelen dar buenos resultados.\n",
    "\n",
    "**¿Debo estandarizar el taraget?** No es lo habitual. Se suele predecir en las unidades del target, pero se podría aplicar el MinMaxScaler y luego la operación inversa tras la predicción. (Ver detalle en el codigo de StandardScaler)\n",
    "\n",
    "**¿Cómo aplico el escalado a train y test?**. Primero creo los objetos scaler con los datos de train (SOLO los de train), y aplica ese scaler tanto a train como a test. Así no contamino el conjunto de train con los datos de test.\n",
    "    \n",
    "**¿Cambia la forma de la distribución con este scaler?** NO. Si tenías una distribución asimétrica, seguirá siendo asimétrica pero con otra escala\n",
    "    \n",
    "**¿Puedo tener valores mayores de 1 o menores que 0?** Lo cierto es que si. Si escalo el conjunto de train con un MinMaxScaler de sklearn, y en el conjunto de test hay valores superiores al máximo del conjunto de train o inferiores al mínimo, habrá valores que se salgan de los límites. Matemáticamente no es un problema grave. Siempre podemos forzar a 0 y 1 si se sale de los límites.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "minmax.fit(X)\n",
    "X_minmax = minmax.transform(X)\n",
    "\n",
    "stdscaler = StandardScaler()\n",
    "stdscaler.fit(X)\n",
    "X_std = stdscaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La forma de las distribuciones no cambia.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original target\n",
    "sns.scatterplot(x = X[:, 0], y = X[:, 1], ax=axes[0])\n",
    "axes[0].set_title(\"Original target\")\n",
    "\n",
    "# MinMaxScaler\n",
    "sns.scatterplot(x =X_minmax[:, 0],  y = X_minmax[:, 1], ax=axes[1])\n",
    "axes[1].set_title(\"MinMaxScaler\")\n",
    "\n",
    "# StandardScaler\n",
    "sns.scatterplot(x =X_std[:, 0],  y = X_std[:, 1], ax=axes[2])\n",
    "axes[2].set_title(\"StandardScaler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feat_reduc\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 15. Feature Reduction\n",
    "<a href=\"#feat_red_prelim\"><p href=\"#feat_red_prelim\">Ver apartado 6</p></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"choose_metric\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 16. Escoger métrica del modelo\n",
    "Por desgracia los modelos no son perfectos y siempre cometen cierto error. De no ser así, probablemente no sería necesario utilizar Machine Learning para solucionar el problema y tengamos que acudir a soluciones más sencillas.\n",
    "\n",
    "Por tanto, tendremos que definir una métrica para evaluar el comportamiento del modelo por dos motivos:\n",
    "1. Primero, para probar varios modelos y poder **comparar** entre ellos cuál es el mejor\n",
    "2. Segundo, para presentar **resultados del modelo**.\n",
    "\n",
    "Prácticamente todas las métricas tratadas en este notebook están implementadas en `sklearn`, salvo MAPE. Consulta [la documentación](https://scikit-learn.org/stable/modules/model_evaluation.html) para más información.\n",
    "\n",
    "### 16.1 Métricas para clasificación\n",
    "\n",
    "##### Accuracy\n",
    "La métrica más habitual. Calcula el % de acierto teniendo en cuenta todas las clases del algoritmo de clasificación. Esta métrica es muy fácil de entender, pero no profundiza en el % de acierto de cada clase, que puede ser algo interesante, dependiendo del problema que queramos tratar.\n",
    "\n",
    "Accuracy = (TP + TN) / Total\n",
    "\n",
    "##### Matriz de confusión\n",
    "Confusión o *error matrix* es una tabla que describe el rendimiento de un modelo supervisado de Machine Learning en los datos de test, donde se desconocen los verdaderos valores. Se llama “matriz de confusión” porque hace que sea fácil detectar dónde el sistema está confundiendo dos clases.\n",
    "\n",
    "* **True Positives (TP)**: cuando la clase real del punto de datos era 1 (Verdadero) y la predicha es también 1 (Verdadero)\n",
    "* **Verdaderos Negativos (TN)**: cuando la clase real del punto de datos fue 0 (Falso) y el pronosticado también es 0 (Falso).\n",
    "* **False Positives (FP)**: cuando la clase real del punto de datos era 0 (False) y el pronosticado es 1 (True).\n",
    "* **False Negatives (FN)**: Cuando la clase real del punto de datos era 1 (Verdadero) y el valor predicho es 0 (Falso).\n",
    "\n",
    "##### Recall o Sensibilidad\n",
    "Los positivos que he clasificado bien vs todos los positivos que había. Métrica que se utiliza cuando queremos ahcer foco en minimizar los FN (Falsos Negativos). Claro ejemplo puede ser un test de COVID. No me importa tanto que haya FP, ya que las consecuencias son aislamientos preventivos, mientras que tener FN, es decir, personas con COVID, cuyo resultado del test es negativo, si es grave ya que pueden producir más contagios.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "##### Precision\n",
    "De los que ha predicho como 1, cuántos en realidad ha acertado. Precision, a diferencia del recall, pone foco en minimizar los FP. Como ejemplo podemos poner un filtro anti-spam. Que se cuele algún correo de spam (FN) no me importa. Ahora bien, si se clasifica como spam un correo importante del jefe (FP) y no lo leemos, sí es más grave.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "##### Specifity\n",
    "Es el número de ítems correctamente identificados como negativos sobre el total de negativos. Es lo opuesto al Recall.\n",
    "\n",
    "Specifity = TN / (TN + FP)\n",
    "\n",
    "##### F1-Score\n",
    "Combinación de las métricas Precision y Recall. Esta métrica se utiliza para comparar clasificadores, ya que es algo más compleja de entender, pero muy útil cuando tenemos clasificadores cuyos valores de recall y precision se intercalan unos con otros y no está muy claro cuál es mejor, a no ser que tengamos claro que hay que centrarse o bien en el recall o en el precision. El rango del F1-score va de 0 a 1, como las métricas anteriores.\n",
    "\n",
    "F1-score = 2 * Precision * Recall / (Precision + Recall)\n",
    "\n",
    "\n",
    "##### Precision vs Recall plot\n",
    "Recuerda que el modelo por defecto calcula las probabilidades de pertenecer a una clase, y que por defecto, si por ejemplo el clasificador es binario, y la probabilidad cae por encima del 50% se clasificaría como 1, como positivo. Ahora bien, modificar ese threshold cambia completamente la matrz de confusión. Si subimos el threshold somos más restrictivos con los 1s, y por tanto empezará a clasificar más valores como 0s y como FN. En cambio si lo bajamos, somos más permisivos con los 1s, los clasificaríamos casi todos, pero subirían los FP.\n",
    "\n",
    "Es por ello que existen curvas como ROC o Precision vs Recall con las que podremos ver diferentes comportamientos del clasificador, teniendo en cuenta varios thresholds, y todo ello en una misma gráfica.\n",
    "\n",
    "Mediante esta curva obtendremos el punto óptimo de precision vs recall. Para usar esta gráfica hay que comprender bien qué significan las métricas de precision y de recall\n",
    "\n",
    "<img src=\"./img/precs_recall.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "##### ROC Curve/AUC\n",
    "Similar a la anterior curva. Nos sirve para comparar el recall con FPR = FP/(FP + TN)\n",
    "\n",
    "De esta curva podemos obtener una medida, que es el AUC (Area Under the Curve). El AUC cuanto más cercano a 1, mejor será el clasificador. Si la curva es una línea recta estaremos ante un clasificador muy malo, que no sería muy diferente a un clasificador aleatorio.\n",
    "\n",
    "**Normalmente se usa la ROC curve cuando tenemos datasets balanceados, mientras que la curva precision-recall es más propia de datasets con una clase mayoritaria.**\n",
    "\n",
    "<img src=\"./img/roc.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metricas de clasificación\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,\\\n",
    "                            roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    "\n",
    "# Modelo rapido sin dividir en train/test ni ndada\n",
    "titanic_df2 = titanic_df.drop(columns=['deck', 'alive']).copy()\n",
    "titanic_df2 = titanic_df2.dropna()\n",
    "titanic_df2 = pd.get_dummies(titanic_df2)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "X = titanic_df2.iloc[:, 1:]\n",
    "Y = titanic_df2.iloc[:, 0]\n",
    "\n",
    "lr.fit(X, Y)\n",
    "preds = lr.predict(X)\n",
    "\n",
    "print(\"Score del modelo (accuracy):\", round(lr.score(X, Y), 3))\n",
    "print(\"Accuracy score:\", round(accuracy_score(preds, Y), 3))\n",
    "print(\"Recall score:\", round(recall_score(preds, Y), 3))\n",
    "print(\"Precision score:\", round(precision_score(preds, Y), 3))\n",
    "print(\"F1 score:\", round(f1_score(preds, Y), 3))\n",
    "print(\"AUC:\", round(roc_auc_score(preds, Y), 3))\n",
    "\n",
    "\n",
    "y_pred_prob = lr.predict_proba(X)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(Y, y_pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for titanic classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(Y, y_pred_prob)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlim([0.0, 1.1])\n",
    "plt.ylim([0.4, 1.1])\n",
    "plt.title('Precision vs Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(Y, preds)\n",
    "print(c_matrix)\n",
    "import seaborn as sns\n",
    "sns.heatmap(c_matrix, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(Y, preds, normalize='true'), annot=True, \n",
    "            fmt='.2%', cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.2 Métricas para regresión\n",
    "\n",
    "##### R^2\n",
    "El R^2 es el coeficiente de determinación, representa el porcentaje de variación de la variable dependiente vs la independiente para un modelo lineal. Se mide entre 0 y 1. El problema es que R^2 no nos indica si mi modelo se ajusta bien a mis datos. Un buen modelo puede tener un R^2 bajo, e incluso un modelo mal entrenado podría llegar a tener un R^2 alto. [En este artículo tienes más detalles](https://statisticsbyjim.com/regression/interpret-r-squared-regression/#:~:text=R%2Dsquared%20evaluates%20the%20scatter,around%20the%20fitted%20regression%20line.&text=For%20the%20same%20data%20set,that%20a%20linear%20model%20explains.)\n",
    "\n",
    "<img src=\"./img/r_squared.png\" alt=\"drawing\" width=\"230\"/>\n",
    "\n",
    "##### Mean Squared Error (MSE)\n",
    "La media de los errores al cuadrado. ¿Por qué al cuadrado? Porque al calcular la suma no queremos que se nos cancelen unos errores con otros. ¿Qué esté al cuadrado nos sirve como medida de errores? Perfectamente, es más, esta métrica magnifica los errores grandes, por lo que si esos son los errores que no corregimos, seguiremos teniendo un MSE alto. \n",
    "\n",
    "MSE es una métrica muy utilizada ya que nos sirve para hacer foco en los errores grandes. Como los datos están al cuadrado, no suele ser una métrica para presentar resultados ya que los datos están \"distorsionados\". **Se utiliza mayormente para comparar modelos**.\n",
    "\n",
    "<img src=\"./img/mse.jpg\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "##### Root Mean Squared Error (RMSE)\n",
    "Igual que en el caso anterior, pero ahora en unidades del target. Esta métrica nos servirá tanto para comparar modelos, como para presentar resultados.\n",
    "\n",
    "<img src=\"./img/rmse.png\" alt=\"drawing\" width=\"180\"/>\n",
    "\n",
    "##### Mean Absolute Error (MAE)\n",
    "Muy util cuando queremos medir errores en las unidades de la variable. Te dice qué error puedes esperar en la predicción de la variable. Muy bueno cuando hay outliers, ya que lo vamos a ver claramente reflejado en la medida.\n",
    "\n",
    "<img src=\"./img/mae.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "##### Mean Absolute Percentage Error (MAPE)\n",
    "Es la única métrica acotada entre 0 y 1. Proporciona, en media, el % de error para cada punto respecto a su true label. Cuando queremos comparar modelos con diferentes targets, como por ejemplo en series temporales, las otras métricas no nos sirven porque van en función de las unidades de cada target, sin embargo mediante el MAPE podremos comparar con la misma unidad de medidas.\n",
    "\n",
    "<img src=\"./img/mape.jpeg\" alt=\"drawing\" width=\"230\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metricas de regresión\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "diamonds_df2 = pd.get_dummies(diamonds_df)\n",
    "\n",
    "lr = LinearRegression()\n",
    "X = diamonds_df2.drop(columns = ['price'])\n",
    "Y = diamonds_df2['price']\n",
    "\n",
    "lr.fit(X, Y)\n",
    "preds = lr.predict(X)\n",
    "\n",
    "print(\"Score del modelo (R^2):\", round(lr.score(X, Y), 4))\n",
    "print(\"R^2 score:\", round(r2_score(preds, Y), 4))\n",
    "print(\"MAE score:\", round(mean_absolute_error(preds, Y), 4))\n",
    "print(\"MSE score:\", round(mean_squared_error(preds, Y), 4))\n",
    "print(\"RMSE score:\", round(np.sqrt(mean_squared_error(preds, Y)), 4))\n",
    "\n",
    "def mean_absolute_percentage_error(y_pred, y_true): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "print(\"MAPE score:\", round(mean_absolute_percentage_error(preds, Y), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"choose_models\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 17. Decidir qué modelos\n",
    "Existen una gran cantidad de modelos que nos solventarán problemas de todo tipo. Ahora bien, ¿qué modelo se adapta mejor a los datos? Va a depender de los siguientes factores:\n",
    "\n",
    "#### Volumen de datos\n",
    "Lo ideal es cuantos más datos mejor, pero si tenemos:\n",
    "1. **Pocos datos y muchas features**: elige algoritmos con alto bias y low variance. Algoritmos que tienen algo más de error, pero generalizan mejor. Linear regression, naive bayes, Linear SVM\n",
    "2. **Muchos datos y pocas features**: elige algoritmos con low bias y high variance: KNN, Decission trees, resto de kernels de SVM\n",
    "\n",
    "#### Accuracy vs interpretability\n",
    "Lo más importante a la hora de elegir modelo. ¿Cuánto de explicable tiene que ser mi modelo? Si tenemos que dar una justificación detallada de las decisiones que toma mi modelo, necesitaré un algoritmo de caja blanca, de lo contrario, será un caja negra. Normalmente los algoritmos caja negra tienen una mejor precisión. El problema que presentan es que resulta muy difícil seguir la traza de los outputs del modelo y no sabemos muy bien qué está haciendo.\n",
    "\n",
    "<img src=\"./img/choose_algorithm.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "\n",
    "#### Velocidad de entrenamiento\n",
    "Normalmente los algoritmos más precisos suelen tener un tiempo de entrenamiento alto.\n",
    "1. Rápidos: Logistic regression, linear models, naive bayes.\n",
    "2. Lentos: SVM (por el tunning de hiperparámetros), RRNN, ensembles.\n",
    "\n",
    "#### Relaciones lineales entre los datos\n",
    "La importancia del análisis exploratorio. Si la relacion con el target es lineal, va a predecir bien un modelo lineal como logistic regression o SVM lineal. Si no tendremos que acudir a otros modelos que permitan modelar relaciones no lineales, como por ejemplo random forest, KNN, kernel SVM o redes neuronales.\n",
    "\n",
    "¿Cómo comprobamos esto? Mediante el análisis exploratorio. Aunque otra opción sería entrenar una regresión lineal y analizar la aleatoriedad de los residuos. Deberían caer en una nube alrededor del 0, sin patrones tipo una parábola.\n",
    "\n",
    "<img src=\"./img/choose_algorithm2.png\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hyperparmeters\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 18. Elegir hiperparámetros\n",
    "Como ya sabes, cada dataset es de su padre y de su madre, y por tanto es imposible determinar el modelo con sus hiperparámetros que mejor se ajusten a los datos. Por tanto, tendremos que probar varias combinaciones. Por suerte `sklearn` tiene una función llamada GridSearchCV que permite probar varias combinaciones de una manera automatizada.\n",
    "\n",
    "Empieza iterando unos pocos hiperparámetros y luego ve subiendo, según los resultados de esa ejecución.\n",
    "\n",
    "En este apartado veremos posibles hiperparámetros a emplear en los algoritmos más utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESION LOGISTICA\n",
    "grid_logreg = {                   \n",
    "                     \"penalty\": [\"l1\",\"l2\"], # Regularizaciones L1 y L2.\n",
    "                     \"C\": [0.1, 0.5, 1.0, 5.0], # Cuanta regularizacion queremos\n",
    "                     \n",
    "                     \"max_iter\": [50,100,500],  # Iteraciones del Gradient Descent. No suele impactar mucho\n",
    "                                                # pero en ocasiones aparecen warnings diciendo que se aumente\n",
    "                     \n",
    "                     \"solver\": [\"liblinear\"]  # Suele ser el más rápido\n",
    "                    }\n",
    "\n",
    "\n",
    "# KNN\n",
    "grid_neighbors = {\"n_neighbors\": [3,5,7,9,11],       # Pares acepta sklearn, pero se suele poner impares, por los empates\n",
    "                  \"weights\": [\"uniform\",\"distance\"]  # Ponderar o no las clasificaciones en \n",
    "                                                     # función de la inversa de la distancia a cada vecino\n",
    "                  }\n",
    "\n",
    "# ARBOL DE DECISION\n",
    "grid_arbol = {\"max_depth\":list(range(1,10)) # Profundidades del árbol. Cuanto más profundo, mas posibilidades de overfitting,\n",
    "                                            # pero  mas preciso en entrenamiento.\n",
    "              }\n",
    "\n",
    "\n",
    "# RANDOM FOREST\n",
    "grid_random_forest = {\"n_estimators\": [120], # El Random Forest no suele empeorar por exceso de\n",
    "                                             # estimadores. A partir de cierto numero no merece la pena\n",
    "                                             # perder el tiempo ya que no mejora mucho más la precisión.\n",
    "                                             # Entre 100 y 200 es una buena cifra\n",
    "\n",
    "                     \n",
    "                     \"max_depth\": [3,4,5,6,10,15,17], # No le afecta tanto el overfitting como al decissiontree.\n",
    "                                                      # Podemos probar mayores profundidades\n",
    "                      \n",
    "                     \"max_features\": [\"sqrt\", 3, 4] # Numero de features que utiliza en cada split.\n",
    "                                                    # cuanto más bajo, mejor generalizará y menos overfitting.\n",
    "                                                    \n",
    "                     }\n",
    "\n",
    "\n",
    "# SVM\n",
    "grid_svm = {\"C\": [0.01, 0.1, 0.3, 0.5, 1.0, 3, 5.0, 15, 30], # Parametro de regularizacion\n",
    "            \"kernel\": [\"linear\",\"poly\",\"rbf\"], # Tipo de kernel, probar varios\n",
    "            \"degree\": [2,3,4,5], # Cuantos grados queremos para el kernel polinomico\n",
    "            \"gamma\": [0.001, 0.1, \"auto\", 1.0, 10.0, 30.0] # Coeficiente de regulaizacion para los kernels rbf, poly y sigmoid\n",
    "           }\n",
    "\n",
    "\n",
    "# GRADIENT BOOSTING\n",
    "grid_gradient_boosting = {\"loss\": [\"deviance\"], # Deviance suele ir mejor.\n",
    "                          \"learning_rate\": [0.05, 0.1, 0.2, 0.4, 0.5],  # Cuanto más alto, mas aporta cada nuevo arbol\n",
    "                          \n",
    "                          \"n_estimators\": [20,50,100,200], # Cuidado con poner muchos estiamdores ya que vamos a\n",
    "                                                           # sobreajustar el modelo\n",
    "                          \n",
    "                          \"max_depth\": [1,2,3,4,5], # No es necesario poner una profundiad muy alta. Cada nuevo\n",
    "                                                    # arbol va corrigiendo el error de los anteriores.\n",
    "                          \n",
    "                          \n",
    "                          \"max_features\": [\"sqrt\", 3, 4], # Igual que en el random forest\n",
    "                          }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipelines\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 19. Definimos pipelines y probamos\n",
    "Ya tenemos los modelos y sus hiperparámetros elegidos, ahora solo queda configurar los pipelines para cada modelo. Por norma general, todo el preprocesado y limpieza de datos ya tiene que estar hecho en este punto, por lo que no será necesario cargar en exceso los pipelines. ¿Qué tienes que configurar en los pipelines?\n",
    "1. **El modelo**: como último paso del pipeline. Puedes configurarle hiperparámetros que no vayas a iterar en la propia declaración del modelo.\n",
    "2. **Escalados**: StandardScaler o MinMaxScaler. Menos en árboles.\n",
    "3. **Feat. reduction**: prueba algun PCA o SelectKBest en algunos de los algoritmos, iterando sus hiperparámetros.\n",
    "4. **Missings**: se suelen tratar antes, pero siempre tienes la opción de incluir un imputador de missings que afecte a todo el dataset.\n",
    "\n",
    "**NOTA**: ten en cuenta que cuando se ejecute la validación cruzada, si imputaste missings con la media, por ejemplo, esta media está calculada sobre todo el conjunto de train, y en la validación cruzada (divide train en train+validation) estamos contaminando los datos de train (de la validacion cruzada) con los de validación. No es algo grave, pero lo correcto es para cada kfold calcular sus estadísticos de train y aplicarlos en train y validation. Esto lo hace solo GridSearchCV siempre y cuando el imputador de missings esté incluido en el pipeline. El problema es que hacer una configuración tan personalizada y adaptada a tus datos en el pipeline conlleva muchísimo trabajo y al final se suelen dejar los pipelines para operaciones de escalado y de feature selection.\n",
    "\n",
    "En el siguiente código tendrás varios ejemplos de declaración de pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Si solo es el modelo, no hará falta meterlo en un pipeline\n",
    "rand_forest = RandomForestClassifier()\n",
    "\n",
    "svm = Pipeline([(\"scaler\",StandardScaler()),\n",
    "                (\"selectkbest\",SelectKBest()),\n",
    "                (\"svm\",SVC())\n",
    "               ])\n",
    "\n",
    "\n",
    "reg_log = Pipeline([(\"imputer\",SimpleImputer()),\n",
    "                    (\"scaler\",StandardScaler()),\n",
    "                    (\"reglog\",LogisticRegression())\n",
    "                   ])\n",
    "\n",
    "'''\n",
    "Para iterar hiperparámetros de varios elementos del pipeline, le ponemos un nombre\n",
    "a cada elemento en el pipeline, por ejemplo 'selectkbest' y 'svm', para luego en el\n",
    "grid de hiperparametros identificar sus respectivos parametros mediante el nombre\n",
    "que le hayamos puesto en el pipeline, dos guines bajos y el nombre del hiperparámetro.\n",
    "'''\n",
    "\n",
    "grid_random_forest = {\"n_estimators\": [120],\n",
    "                     \"max_depth\": [3,4,5,6,10,15,17],\n",
    "                     \"max_features\": [\"sqrt\", 3, 4]                          \n",
    "                     }\n",
    "\n",
    "\n",
    "svm_param = {                    \n",
    "            'selectkbest__k': [1,2,3],\n",
    "            'svm__C': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n",
    "            'svm__kernel': [\"linear\",\"poly\",\"rbf\"],\n",
    "            'svm__coef0': [-10.,-1., 0., 0.1, 0.5, 1, 10, 100],\n",
    "            'svm__gamma': ('scale', 'auto')\n",
    "            }\n",
    "\n",
    "\n",
    "reg_log_param = {    \n",
    "                 \"imputer__strategy\": ['mean', 'median', 'most_frequent'],\n",
    "                 \"reglog__penalty\": [\"l1\",\"l2\"], \n",
    "                 \"reglog__C\": np.logspace(0, 4, 10)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almaceno en una lista de tuplas los modelos (nombre que le pongo, el modelo, hiperparametros)\n",
    "models = [('rand_forest', rand_forest, grid_random_forest),\n",
    "         ('svm', svm, svm_param),\n",
    "         ('reg_log', reg_log, reg_log_param)]\n",
    "\n",
    "# Declaro en un diccionario los pipelines e hiperparametros\n",
    "models_gridsearch = {}\n",
    "\n",
    "for i in models:\n",
    "    models_gridsearch[i[0]] = GridSearchCV(i[1],\n",
    "                                          i[2],\n",
    "                                          cv=10,\n",
    "                                          scoring=\"accuracy\",\n",
    "                                          verbose=1,\n",
    "                                          n_jobs=-1)\n",
    "    \n",
    "    models_gridsearch[i[0]].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grids = [(i, j.best_score_) for i, j in models_gridsearch.items()]\n",
    "\n",
    "best_grids = pd.DataFrame(best_grids, columns=[\"Grid\", \"Best score\"]).sort_values(by=\"Best score\", ascending=False)\n",
    "best_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor ha sido la regresion logistica. Ya esta entrenada con todo train\n",
    "models_gridsearch['reg_log'].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La probamos en test\n",
    "models_gridsearch['reg_log'].best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "<a href=\"#init\"><p style=\"text-align:right;\" href=\"#init\">Volver al índice</p></a> \n",
    "# 20. Resultados\n",
    "Una vez elegidas las métricas y evaluados varios modelos, ya tendríamos los resultados del modelo. Sólo queda interpretarlos. Por desgracia este punto va a depender mucho del negocio.\n",
    "1. **Evaluar los resutlados en test**: tenemos que comprobar que los resultados no difieren mucho del entrenamiento. Si son inferiores es debido a que el modelo está sobreentrenado (ver abajo para solucionarlo). Si son superiores es raro, probablemente porque no tengamos muchos datos. Habría que intentar coseguir más observaciones.\n",
    "2. Evaluar los resultados respecto a las **necesidades de negocio**\n",
    "3. **Almacenar** todas las muestras utilizadas en el entrenamiento, así como los scripts y el propio modelo entrenado. La puesta en producción no es el objetivo a cubrir en este notebook.\n",
    "4. Elegir y evaluar la/las métrica/s con las que **presentaremos los resultados del modelo**. No tenemos por qué elegir la misma métrica utilizada en la búsqueda del mejor modelo, ya que no siempre es la más entendible: MSE o AUC.\n",
    "\n",
    "#### ¿Qué hacer si tenemos overfitting?\n",
    "Posibles opciones para reducir el overfitting\n",
    "1. **Cross validation**: si hemos hecho bien el paso anterior, el cross validation es la mejor técnica para evitar el overfitting.\n",
    "2. **Entrenar con más datos**: no quitar datos a test, sino intentar conseguir más datos.\n",
    "3. **Eliminar features**: PCA o un algoritmo de feature selection podría ser una buena solución\n",
    "4. **Max depth**: reducirlo si estamos con árboles\n",
    "5. **Regularización**: aumentarla en los modelos que permitan regulaización como regresiones lineales y SVM\n",
    "6. **Ensembles**: sobretodo los algoritmos de bagging como el random forest no suelen sobreajustarse tanto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "import pickle\n",
    "\n",
    "with open('finished_model.model', \"wb\") as archivo_salida:\n",
    "    pickle.dump(models_gridsearch['reg_log'].best_estimator_, archivo_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para volver a leer el modelo\n",
    "with open('finished_model.model', \"rb\") as archivo_entrada:\n",
    "    pipeline_importada = pickle.load(archivo_entrada)\n",
    "    \n",
    "print(pipeline_importada)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec2a379ed5c25334a484232182c9d38ef8bd9861e2542d0c517568c4f99a9a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
