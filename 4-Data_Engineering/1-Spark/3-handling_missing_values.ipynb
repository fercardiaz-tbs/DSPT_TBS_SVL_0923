{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded0783b",
   "metadata": {},
   "source": [
    "### Pyspark Handling Missing Values\n",
    "- Eliminar columnas\n",
    "- Eliminar Filas\n",
    "- Parámetros para el Dropping\n",
    "- Imputar valores nulos con la media, la mediana y la moda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3cadd",
   "metadata": {},
   "source": [
    "#### CREAMOS LA SESIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e1b5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/03/06 20:38:42 WARN Utils: Your hostname, DESKTOP-8EEJDSD resolves to a loopback address: 127.0.1.1; using 172.28.93.32 instead (on interface eth0)\n",
      "24/03/06 20:38:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/06 20:38:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/06 20:38:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e548e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.28.93.32:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7e9b3506d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885555b9",
   "metadata": {},
   "source": [
    "#### CARGAMOS EL DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5dfac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('data/test2.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cad078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9855000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5991f",
   "metadata": {},
   "source": [
    "La función drop() tiene varios parámetros, entre ellos \"how\", con valores \"any\" y \"all\".\n",
    "- how = 'any' elimina la columna si uno de los valores es nulo (any por defecto)\n",
    "- how = 'all' elimina la columna si todos los valores son nulos\n",
    "\n",
    "Otro parámetro es \"thresh\". Por ejemplo: thresh = 2, significa que se mantienen las filas con al menos 2 valores NO NULOS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433eb8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'any', thresh = 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4588fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'any', thresh = 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca81f14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'any', thresh = 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535f4c1",
   "metadata": {},
   "source": [
    "El último parámetro es \"subset\". Con este, podemos decirle que elimine las columnas con valores nulos en una columna determinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.drop(how = 'any', subset = ['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef0db6",
   "metadata": {},
   "source": [
    "RELLENAR MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1be913ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|         Name| age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|        Krish|  31|        10| 30000|\n",
      "|    Sudhanshu|  30|         8| 25000|\n",
      "|        Sunny|  29|         4| 20000|\n",
      "|         Paul|  24|         3| 20000|\n",
      "|       Harsha|  21|         1| 15000|\n",
      "|      Shubham|  23|         2| 18000|\n",
      "|       Mahesh|NULL|      NULL| 40000|\n",
      "|Missing Value|  34|        10| 38000|\n",
      "|Missing Value|  36|      NULL|  NULL|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Value').show() \n",
    "\n",
    "# también se podría poner df_pyspark.fillna().\n",
    "# Solo rellenará las columnas que comparten el tipo de valor. Si lo que queremos introducir es un string, solo lo sustituirá en aquellas\n",
    "# columnas que sean string. Esto es igual con las columnas numéricas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3cb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distintos tipos en las columnas seleccionadas.\n",
    "\n",
    "df_pyspark.na.fill({'Name':'Missing Value', 'Experience':10}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5313f9d4",
   "metadata": {},
   "source": [
    "#### IMPUTAR VALORES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8b4bf",
   "metadata": {},
   "source": [
    "Podemos sustituir los valores por el valor medio, mediana... de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3ebf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "373d142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mean = Imputer(inputCols=['age', 'Experience', 'Salary'],\n",
    "                    outputCols = ['{}_imputed'.format(c) for c in ['age', 'Experience', 'Salary']]\n",
    "                    ).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d736a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|         28|                 5|         40000|\n",
      "|     NULL|  34|        10| 38000|         34|                10|         38000|\n",
      "|     NULL|  36|      NULL|  NULL|         36|                 5|         25750|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer_mean.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150dd612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "90139cb9a825bf3d63f6f6704e828dbd1ff7edbd4d0c6e906a71235d6efc74af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
